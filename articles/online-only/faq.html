<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Cross-validation FAQ • loo</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><meta property="og:title" content="Cross-validation FAQ">
<meta property="og:description" content="loo">
<meta property="og:image" content="https://mc-stan.org/loo/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">loo</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.6.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../../reference/index.html">Functions</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Other Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://mc-stan.org/rstan" class="external-link">rstan</a>
    </li>
    <li>
      <a href="https://mc-stan.org/cmdstanr" class="external-link">cmdstanr</a>
    </li>
    <li>
      <a href="https://mc-stan.org/rstanarm" class="external-link">rstanarm</a>
    </li>
    <li>
      <a href="https://mc-stan.org/bayesplot" class="external-link">bayesplot</a>
    </li>
    <li>
      <a href="https://mc-stan.org/shinystan" class="external-link">shinystan</a>
    </li>
    <li>
      <a href="https://mc-stan.org/projpred" class="external-link">projpred</a>
    </li>
    <li>
      <a href="https://mc-stan.org/rstantools" class="external-link">rstantools</a>
    </li>
    <li>
      <a href="https://mc-stan.org/posterior" class="external-link">posterior</a>
    </li>
  </ul>
</li>
<li>
  <a href="https://mc-stan.org" class="external-link">Stan</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://twitter.com/mcmc_stan" class="external-link">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/stan-dev/loo" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://discourse.mc-stan.org/" class="external-link">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Cross-validation FAQ</h1>
                        <h4 data-toc-skip class="author"><a href="https://users.aalto.fi/~ave/" class="external-link">Aki Vehtari</a></h4>
            
            <h4 data-toc-skip class="date">First version 2020-03-11.
Last modified 2023-03-31.</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/stan-dev/loo/blob/HEAD/vignettes/online-only/faq.Rmd" class="external-link"><code>vignettes/online-only/faq.Rmd</code></a></small>
      <div class="hidden name"><code>faq.Rmd</code></div>

    </div>

    
    
<p>Here are some answers by <a href="https://users.aalto.fi/~ave/" class="external-link">Aki
Vehtari</a> to frequently asked questions about cross-validation and
<code>loo</code> package. If you have further questions, please ask them
in <a href="https://discourse.mc-stan.org/t/cross-validation-faq/13664" class="external-link">Stan
discourse thread named Cross-validation FAQ</a>.</p>
<hr>
<div class="section level2">
<h2 id="whatis">What is cross-validation?<a class="anchor" aria-label="anchor" href="#whatis"></a>
</h2>
<p>Cross-validation is a family of techniques that try to estimate how
well a model would predict previously unseen data by using fits of the
model to a subset of the data to predict the rest of the data.</p>
<p>Cross-validation can be used to:</p>
<ul>
<li>Asses the predictive performance of a single model</li>
<li>Asses model misspecification or calibration of the predictive
distribution of a single model</li>
<li>Compare multiple models</li>
<li>Select a single model from multiple candidates</li>
<li>Combine the predictions of multiple models</li>
</ul>
<p>Even if the goal of the model is not to make predictions, a model
which makes bad or badly calibrated predictions is less likely to
provide useful insights to a phenomenon studied.</p>
<div class="section level3">
<h3 id="using-cross-validation-for-a-single-model">Using cross-validation for a single model<a class="anchor" aria-label="anchor" href="#using-cross-validation-for-a-single-model"></a>
</h3>
<p>Two basic cases why to use cross-validation for one model are:</p>
<ol style="list-style-type: decimal">
<li>We want to know how good predictions the model can make for future
or otherwise unseen observations.</li>
<li>We want to know if the model describes the observed data well, but
we are not going make any predictions for the future.</li>
</ol>
<p>More about these cases:</p>
<p>1 ) For example, <span class="citation">Vehtari and Lampinen (<a href="#ref-Vehtari+Lampinen:2002b" role="doc-biblioref">2002</a>)</span>
describe a model for predicting concrete quality based on amount of
cement, sand properties (see <span class="citation">Kalliomäki, Vehtari
and Lampinen (<a href="#ref-Kalliomaki+Vehtari+Lampinen:2005" role="doc-biblioref">2005</a>)</span>), and additives. One of the
quality measurements is compressive strength 3 months after casting. For
example, when constructing bridges, it is very useful to be able to
predict the compressive strength before casting concrete. <span class="citation">Vehtari and Lampinen (<a href="#ref-Vehtari+Lampinen:2002b" role="doc-biblioref">2002</a>)</span>
estimated 90% quantile of absolute error for new castings, that is, they
reported that in 90% of cases the difference between the prediction and
the actual measurement 3 months after the casting is less than the given
value (also other quantiles were reported to the concrete experts). This
way it was possible to asses whether the prediction model was accurate
enough to have practical relevance.</p>
<p>2a) Even if we are not interested in predicting the actual future, a
model which could make good predictions has learned something useful
from the data. For example, if a regression model is not able to predict
better than null model (a model only for the marginal distribution of
the data) then it has not learned anything useful from the predictors.
Correspondingly for time series models the predictors for the next time
step can be observation values in previous time steps.</p>
<p>2b) Instead of considering predictions for future, we can consider
whether we can generalize from some observations to others. For example,
in social science we might make a model explaining poll results with
demographical data. To test the model, instead of considering future
pollings, we could test whether the model can predict for a new state.
If we have observed data from all states in USA, then there are no new
states (or it can take unpredictable time before there are new states),
but we can simulate a situation where we leave out data from one state
and check can we generalize from other states to the left out state.
This is sensible approach when we assume that states are exchangeable
conditional on the information available (see, e.g., <span class="citation">Gelman <em>et al.</em> (<a href="#ref-BDA3" role="doc-biblioref">2013</a>)</span> Chapter 5 for exchangeability).
The generalization ability from one entity (a person, state, etc) to
other similar entity tells us that model has learned something useful.
It is very important to think what is the level where the generalization
is most interesting. For example, in cognitive science and psychology it
would be more interesting to generalize from one person to another than
within person data from one trial to another trial for the same person.
In cognitive science and psychology studies it is common that the study
population is young university students, and in such thus there are
limitations what we can say about the generalization to whole human
population. In polling data from all US states, the whole population of
US states has been observed, but there is limitation how we can
generalize to other countries or future years.</p>
<p>2c) In addition of assessing the predictive accuracy and
generalizability, it is useful to assess how well calibrated is the
uncertainty quantification of the predictive distribution.
Cross-validation is useful when we don’t trust that the model is well
specified, although many bad mis-specifications can be diagnosed also
with simpler posterior predictive checking. See, for example, case study
<a href="https://avehtari.github.io/modelselection/roaches.html" class="external-link">roaches</a>.</p>
</div>
<div class="section level3">
<h3 id="manymodels">Using cross-validation for many models<a class="anchor" aria-label="anchor" href="#manymodels"></a>
</h3>
<p>Three basic cases for why to use cross-validation for many models
are:</p>
<ol style="list-style-type: decimal">
<li>We want to use the model with best predictions.</li>
<li>We want to use the model which has learned most from the data and is
providing best generalization between interesting entities.</li>
<li>We want combine predictions of many models, weighted by the
estimated predictive performance of each model.</li>
</ol>
<p>More about these cases:</p>
<p>1 ) Use of cross-validation to select the model with best predictive
performance is relatively safe if there are small or moderate number of
models, and there is a lot of data compared to the model complexity or
the best model is clearly best <span class="citation">Sivula, Magnusson
and Vehtari (<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">2020</a>)</span>. See also Section <a href="#modelselection">How to use cross-validation for model
selection?</a>.</p>
<p>2a) Cross-validation is useful especially when there are posterior
dependencies between parameters and examining the marginal posterior of
a parameter is not very useful to determine whether the component
related to that parameter is relevant. This happens, for example, in
case of collinear predictors. See, for example, case studies <a href="https://avehtari.github.io/modelselection/collinear.html" class="external-link">collinear</a>,
<a href="https://avehtari.github.io/modelselection/mesquite.html" class="external-link">mesquite</a>,
and <a href="https://avehtari.github.io/modelselection/bodyfat.html" class="external-link">bodyfat</a>.</p>
<p>2b) Cross-validation is less useful for simple models with no
posterior dependencies and assuming that simple model is not
mis-specified. In that case the marginal posterior is less variable as
it includes the modeling assumptions (which assume to be not
mis-specified) while cross-validation uses non-model based approximation
of the future data distribution which increases the variability. See,
for example, case study <a href="https://avehtari.github.io/modelselection/betablockers.html" class="external-link">betablockers</a>.</p>
<p>2c) Cross-validation can provide quantitative measure, which should
only complement but not replace understanding of qualitative patterns in
the data (see, e.g., <span class="citation">Navarro (<a href="#ref-Navarro:2019:between" role="doc-biblioref">2019</a>)</span>).</p>
<p>3 ) See more in <a href="#modelaveraging">How to use cross-validation
for model averaging?</a>.</p>
<p>See also the next Section “When not to use cross-validation?”, [How
is cross-validation related to overfitting?}(#overfitting), and <a href="#modelselection">How to use cross-validation for model
selection?</a>.</p>
</div>
<div class="section level3">
<h3 id="when-not-to-use-cross-validation">When not to use cross-validation?<a class="anchor" aria-label="anchor" href="#when-not-to-use-cross-validation"></a>
</h3>
<p>In general there is no need to do any model selection (see more in <a href="#overfitting">How is cross-validation related to overfitting?</a>,
and <a href="#modelselection">How to use cross-validation for model
selection?</a>). The best approach is to build a rich model that
includes all the uncertainties, do model checking, and possible model
adjustments.</p>
<p>Cross-validation cannot answer directly the question “Do the data
provide evidence for some effect being non-zero?” Using cross-validation
to compare a model with an additional term to a model without that term
is a kind of null hypothesis testing. Cross-validation can tell whether
that extra term can improve the predictive accuracy. The improvement in
the predictive accuracy is a function of signal-to-noise-ratio, the size
of the actual effect, and how much the effect is correlating with other
included effects. If cross-validation prefers the simpler model, it is
not necessarily evidence for an effect being exactly zero, but it is
possible that the effect is too small to make a difference, or due to
the dependencies it doesn’t provide additional information compared to
what is already included in the model. Often it makes more sense to just
fit the larger model and explore the posterior of the relevant
coefficient. Analysing the posterior can however be difficult if there
are strong posterior dependencies.</p>
<p>Cross-validation is not good for selecting a model from a large
number of models (see <a href="#modelselection">How to use
cross-validation for model selection?</a>)</p>
</div>
</div>
<div class="section level2">
<h2 id="tutorial-material-on-cross-validation">Tutorial material on cross-validation<a class="anchor" aria-label="anchor" href="#tutorial-material-on-cross-validation"></a>
</h2>
<ul>
<li>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian
model evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413–1432. <a href="http://link.springer.com/article/10.1007%2Fs11222-016-9696-4" class="external-link">online</a>.</li>
<li><a href="https://mc-stan.org/loo/reference/loo-glossary.html">LOO
glossary</a></li>
<li>
<a href="https://avehtari.github.io/modelselection/" class="external-link">Model selection
video lectures</a> and Bayesian Data Analysis lectures <a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d7849131-0afd-4ae6-ad64-aafb00da36f4" class="external-link">8.2</a>,
<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=50b2e73f-af0a-4715-b627-ab0200ca7bbd" class="external-link">9.1</a>,
<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b0299d53-9454-4e33-9086-ab0200db14eeb" class="external-link">9.2</a>.</li>
<li>Decision theoretical background on Bayesian cross-validation can be
found in the article <a href="https://dx.doi.org/10.1214/12-SS102" class="external-link">A
survey of Bayesian predictive methods for model assessment, selection
and comparison</a> <span class="citation">(<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">Vehtari and Ojanen,
2012</a>)</span>.</li>
</ul>
</div>
<div class="section level2">
<h2 id="parts">What are the parts of cross-validation?<a class="anchor" aria-label="anchor" href="#parts"></a>
</h2>
<p>It is important to separate</p>
<ol style="list-style-type: decimal">
<li>the way how the data is divided in cross-validation,
e.g. leave-one-out (LOO), leave-one-group-out (LOGO), and
leave-future-out (LFO)</li>
<li>the utility or loss, e.g. expected log predictive density (ELPD),
root mean square error (RMSE), explained variance (<span class="math inline">\(R^2\)</span>)</li>
<li>the computational method use to compute leave-one-out predictive
distributions, e.g. K-fold-CV, Pareto smoothed importance sampling
(PSIS),</li>
<li>and the estimate obtained by combining these.</li>
</ol>
<div class="section level3">
<h3 id="the-way-how-the-data-is-divided-in-cross-validation">The way how the data is divided in cross-validation<a class="anchor" aria-label="anchor" href="#the-way-how-the-data-is-divided-in-cross-validation"></a>
</h3>
<p>Different partitions of data are held out in different kinds of
cross-validation.</p>
<ul>
<li>CV: cross-validation approach (no specific partition defined)</li>
<li>LOO or LOO-CV: leave-one-out cross-validation approach (single
observation)</li>
<li>LFO: leave-future-out cross-validation approach (all future
observations). See more in <a href="#timeseries">Can cross-validation be
used for time series?</a>.</li>
<li>LOGO: leave-one-group-out cross-validation approach (a group of
observations). See more in <a href="#hierarchical">Can cross-validation
be used for hierarchical / multilevel models?</a>.</li>
</ul>
<p>Which unit is systematically left out determines the predictive task
that cross-validation assesses model performance on (see more in <a href="#valid">When is cross-validation valid?</a>). CV, LOO, LFO and
LOGO and other cross-validation approaches do not yet specify the
utility or loss, or how the computation is made except that it involves
estimating cross-validated predictive densities or probabilities.</p>
</div>
<div class="section level3">
<h3 id="the-utility-or-loss">The utility or loss<a class="anchor" aria-label="anchor" href="#the-utility-or-loss"></a>
</h3>
<p>First we need to define the utility or loss function which compares
predictions to observations. These predictions can be considered to be
for future observations, or for other exchangeable entities (see more in
<a href="#id_%C2%A4whatis">What is cross-validation?</a>). Some examples:</p>
<ul>
<li>LPD or LPPD: Log pointwise predictive density for a new observation.
For simplicity the LPD acronym is used also for expected log pointwise
predictive probabilities for discrete models. Often a shorter term log
score is used.</li>
<li>RMSE: Root mean square error.</li>
<li>ACC: Classification accuracy.</li>
<li>
<span class="math inline">\(R^2\)</span>: Explained variance (see,
e.g., <span class="citation">Gelman <em>et al.</em> (<a href="#ref-Gelman+etal:2019:BayesR2" role="doc-biblioref">2019</a>)</span>)</li>
<li>90% quantile of absolute error (see, e.g., <span class="citation">Vehtari and Lampinen (<a href="#ref-Vehtari+Lampinen:2002b" role="doc-biblioref">2002</a>)</span>)</li>
</ul>
<p>These are examples of utility and loss functions for using the model
to predict the future data and then observing that data. Other utility
and loss functions could also be used. See more in <a href="#otherutility">Can other utilities or losses be used than log
predictive density?</a>, <a href="https://en.wikipedia.org/wiki/Scoring_rule" class="external-link">Scoring rule in
Wikipedia</a>, and <a href="https://doi.org/10.1198/016214506000001437" class="external-link">Gneiting and Raftery,
2012</a>.</p>
<p>The value of the loss functions necessarily depends on the data we
observe next. We can however try to estimate an <em>expectation</em> of
the loss (a summary of average predictive performance over several
predictions or expected predictive performance for one prediction) under
the assumption that both the covariates and responses we currently have
are representative of those we will observe in the future.</p>
<ul>
<li>ELPD: The theoretical expected log pointwise predictive density for
new observations (or other exchangeable entity) (Eq 1 in <span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>). One scenario when we could also
actually observe this is if we would get infinite number of future
observations from the same data generating mechanism. However, this
expected value is valid also when thinking just about one future
observation (other exchangeable entity). This can be computed given
different data partitions. For simplicity the ELPD acronym is used also
for expected log pointwise predictive probabilities for discrete
models.</li>
</ul>
<p>Similarly we can have expected RMSE, ACC, <span class="math inline">\(R^2\)</span>, etc.</p>
</div>
<div class="section level3">
<h3 id="combination-of-data-division-and-utility-loss">Combination of data division and utility / loss<a class="anchor" aria-label="anchor" href="#combination-of-data-division-and-utility-loss"></a>
</h3>
<p>In the papers and <code>loo</code> package, following notations have
been used</p>
<ul>
<li>elpd_loo: The Bayesian LOO estimate of the expected log pointwise
predictive density (Eq 4 in <span class="citation">Vehtari, Gelman and
Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>).</li>
<li>elpd_lfo: The Bayesian LFO estimate of the expected log pointwise
predictive density (see, e.g, <span class="citation">Bürkner, Gabry and
Vehtari (<a href="#ref-Burkner+Gabry+Vehtari:LFO-CV:2020" role="doc-biblioref">2020</a>)</span>).</li>
<li>LOOIC: -2*elpd_loo. See later for discussion of multiplier -2.</li>
<li>p_loo: This is not utility/loss as the others, but an estimate of
effective complexity of the model, which can be used for diagnostics.
See Vignette <a href="https://mc-stan.org/loo/reference/loo-glossary.html">LOO
Glossary</a> for interpreting p_loo when Pareto k is large.</li>
</ul>
<p>Similarly we can use the similar notation for other data divisions,
and utility and loss functions. For example, when using LOO data
division</p>
<ul>
<li>eRMSE_loo: The Bayesian LOO estimate of the expected root mean
square error (RMSE).</li>
<li>eACC_loo: The Bayesian LOO estimate of the expected classification
accuracy (ACC).</li>
<li>e<span class="math inline">\(R^2\)</span>_loo: The Bayesian LOO
estimate of the expected explained variance (<span class="math inline">\(R^2\)</span>).</li>
</ul>
<p>These terms are not yet defining possible computational
approximations.</p>
</div>
<div class="section level3">
<h3 id="the-computational-method-used-to-compute-leave-one-out-predictive-distributions">The computational method used to compute leave-one-out predictive
distributions<a class="anchor" aria-label="anchor" href="#the-computational-method-used-to-compute-leave-one-out-predictive-distributions"></a>
</h3>
<p>The choice of partitions to leave out or metric of model performance
is independent of the computational method (e.g. PSIS or K-fold-CV).
Different computational methods can be used to make the computation
faster:</p>
<ul>
<li>K-fold-CV: Each cross-validation fold uses the same inference as is
used for the full data. For example, if MCMC is used then MCMC inference
needs to be run K times.</li>
<li>LOO with K-fold-CV: If K=N, where N is the number of observations,
then K-fold-CV is LOO. Sometimes this is called exact, naive or
brute-force LOO. This can be time consuming as the inference needs to be
repeated N times.</li>
<li>PSIS-LOO: Pareto smoothed importance sampling leave-one-out
cross-validation. Pareto smoothed importance sampling (PSIS, <span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>, <span class="citation">Vehtari
<em>et al.</em> (<a href="#ref-Vehtari+etal:PSIS:2019" role="doc-biblioref">2019</a>)</span>) is used to estimate leave-one-out
predictive densities or probabilities.</li>
<li>PSIS: Richard McElreath shortens PSIS-LOO as PSIS in Statistical
Rethinking, 2nd ed.</li>
<li>MM-LOO: Moment matching importance sampling leave-one-out
cross-validation <span class="citation">(<a href="#ref-Paananen+etal:2021:implicit" role="doc-biblioref">Paananen
<em>et al.</em>, 2021</a>)</span>. Which works better than PSIS-LOO in
challenging cases, but is still faster than K-fold-CV with K=N.</li>
<li>RE-LOO: Run exact LOO (see LOO with K-fold-CV) for those
observations for which PSIS diagnostic indicates PSIS-LOO is not
accurate (that is, re-fit the model for those leave-one-out cases).</li>
</ul>
<p>We could write elpd_{psis-loo}, but often drop the specific
computational method and report diagnostic information only if that
computation may be unreliable.</p>
</div>
<div class="section level3">
<h3 id="the-estimate-obtained-by-combining-these">The estimate obtained by combining these<a class="anchor" aria-label="anchor" href="#the-estimate-obtained-by-combining-these"></a>
</h3>
<p>When discussing, for example, properties of elpd_loo computed with
PSIS-LOO, we can separately discuss limitations of - ELPD: Is this
useful to know in the specific modeling task? Is log score the most
appropriate utility or loss function in this modeling task? - LOO: Is
LOO valid? Is LOO matching the interesting predictive task? Is inherent
variance of LOO estimate problematic? - PSIS: Is IS failing? Is PSIS
failing? Is additional variability due to computational approximation of
LOO problematic?</p>
</div>
</div>
<div class="section level2">
<h2 id="overfitting">How is cross-validation related to overfitting?<a class="anchor" aria-label="anchor" href="#overfitting"></a>
</h2>
<p>Statistical folklore says we need cross-validation to avoid
overfitting. Overfitting can refer to different things:</p>
<ol style="list-style-type: decimal">
<li>When we want to estimate the expected predictive performance for a
new dataset, there are bad estimators which are said to give overfitted
estimated of the predictive performance.</li>
<li>Folklore says that due to overfitting a bigger more complex model
may have worse predictive performance than a smaller simpler model.</li>
<li>How we make the inference for model parameters can make the model
overfit.</li>
</ol>
<p>More about these cases:</p>
<ol style="list-style-type: decimal">
<li><p>If we condition the model on data, then make predictions for that
same data, and finally compute expected utility or loss by comparing the
predictions to that same data, the estimate is overoptimistic as the
model has been fitted to the data. We want the model to fit to the data,
but it’s not clear how much fitting is overfitting. Cross-validation is
able to estimate better the predictive performance for future or
otherwise unseen data (or exchangeable entities) and can be also used to
assess how much model has fitted to the data.</p></li>
<li><p>Overfitting of bigger more complex models is a bigger problem
when using less good inference methods. For example, bigger models
fitted using maximum likelihood can have much worse predictive
performance than simpler models. Overfitting of bigger models is a
smaller problem in Bayesian inference because a) integrating over the
posterior and b) use of priors. The impact of integration over the
posterior is often underestimated compared to the impact of priors. See
<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=75b9f18f-e379-4557-a5fa-a9f500f11b40" class="external-link">a
video demonstrating how integration over the posterior is also
regularizing and reducing overfit</a>. It is still possible to make
Bayesian models to overfit by using priors which have much more
probability mass for over-complex solutions instead of simple solutions.
Combination of (accurate) integration and sensible priors make it common
that, for example, adding more predictors doesn’t decrease the
predictive performance of bigger models even if the number of predictors
is much higher than the number of observations (which would be a big
problem with maximum likelihood).</p></li>
<li><p>In (2), it was mentioned that when using maximum likelihood tends
to overfit more easily. In Bayesian inference, using approximate
integration, for example, using variational inference with normal
distribution with diagonal covariance, can overfit more than when using
accurate integration. If accurate Bayesian inference is used for each
model, but model selection is made using, for example, cross-validation,
then the model selection process can overfit badly <span class="citation">(<a href="#ref-Piironen+Vehtari:2017a" role="doc-biblioref">Piironen and Vehtari, 2017</a>)</span>.</p></li>
</ol>
</div>
<div class="section level2">
<h2 id="modelselection">How to use cross-validation for model selection?<a class="anchor" aria-label="anchor" href="#modelselection"></a>
</h2>
<p>Summary</p>
<ul>
<li>First avoid model selection by using the model which includes all
predictors and includes all uncertain things. Then optimal thing is to
integrate over all the uncertainties. When including many components to
a model, it is useful to think more carefully about the prior. For
example, if there are many predictors, it is useful to use priors that
a) state that only some of the effects are big, or b) many effects are
big and correlating (it is not possible to have a large number of big
independent effects <span class="citation">Tosh <em>et al.</em> (<a href="#ref-Tosh+etal:2021:piranha" role="doc-biblioref">2021</a>)</span>).</li>
<li>If there is explicit utility or loss for observing future predictor
values (e.g. medical tests) use decision theory.</li>
<li>If there is implicit cost for bigger models (e.g. bigger model more
difficult to explain or costs of feature measurements are unknown),
choose a smaller model which similar predictive performance as the
biggest model. If there are only a small number of models, overfitting
due to selection process is small. If there are a large number of
models, as for example often in variable selection, then the overfitting
due to the selection process can be a problem <span class="citation">(<a href="#ref-Piironen+Vehtari:2017a" role="doc-biblioref">Piironen and
Vehtari, 2017</a>)</span> and more elaborate approaches, such as
projection predictive variable selection is recommended.</li>
<li>If there is application specific utility or loss function, use that
to assess practically relevant difference in predictive performance of
two models.</li>
<li>If there is no application specific utility or loss function, use
log score, ie elpd. If elpd difference (elpd_diff in <code>loo</code>
package) is less than 4, the difference is small <span class="citation">(<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">Sivula, Magnusson and Vehtari, 2020</a>)</span>).
If elpd difference (elpd_diff in loo package) is larger than 4, then
compare that difference to standard error of elpd_diff (provided e.g. by
<code>loo</code> package) <span class="citation">(<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">Sivula, Magnusson and Vehtari, 2020</a>)</span>.
See also Section <a href="#se_diff">How to interpret in Standard error
(SE) of elpd difference (elpd_diff)?</a>.</li>
</ul>
<p>If there is a large number of models compared, there is possibility
of overfitting in model selection.</p>
<ul>
<li>See video <a href="https://www.youtube.com/watch?v=Re-2yVd0Mqk" class="external-link">Model assessment,
comparison and selection at Master class in Bayesian statistics, CIRM,
Marseille</a>.</li>
<li>
<span class="citation">Vehtari and Ojanen (<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">2012</a>)</span>
write: “The model selection induced bias can be taken into account by
the double/nested/2-deep cross-validation (e.g. Stone, 1974; Jonathan,
Krzanowski and McCarthy, 2000) or making an additional bias correction
(Tibshirani and Tibshirani, 2009).”</li>
<li>
<span class="citation">Piironen and Vehtari (<a href="#ref-Piironen+Vehtari:2017a" role="doc-biblioref">2017</a>)</span>
write: “Although LOO-CV and WAIC can be used to obtain a nearly unbiased
estimate of the predictive ability of a given model, both of these
estimates contain a stochastic error term whose variance can be
substantial when the dataset is not very large. This variance in the
estimate may lead to over-fitting in the selection process causing
nonoptimal model selection and inducing bias in the performance estimate
for the selected model (e.g., Ambroise and McLachlan 2002; Reunanen
2003; Cawley and Talbot 2010). The overfitting in the selection may be
negligible if only a few models are being compared but, as we will
demonstrate, may become a problem for a larger number of candidate
models, such as in variable selection.”</li>
<li>Nested CV helps to estimate the overfitting due to the selection but
doesn’t remove that.</li>
<li>The overfitting is more severe depending on how many degrees of
freedom there are in the selection. For example, in predictor selection
we can think that we as many indicator variables as there are predictors
and then there are combinatorial explosion in possible parameter
combinations and overfitting can be severe (as demonstrated by <span class="citation">Piironen and Vehtari (<a href="#ref-Piironen+Vehtari:2017a" role="doc-biblioref">2017</a>)</span>).</li>
</ul>
<p>Thus if there are a very large number of models to compared, more
elaborate approaches are recommended such as projection predictive
variable selection [<span class="citation">Piironen and Vehtari (<a href="#ref-Piironen+Vehtari:2017a" role="doc-biblioref">2017</a>)</span>; Piironen+etal:projpred:2020].</p>
<p>See more in tutorial videos on using cross-validation for model
selection</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=Re-2yVd0Mqk" class="external-link">Model
assessment, comparison and selection at Master class in Bayesian
statistics, CIRM, Marseille</a></li>
<li>Bayesian data analysis lectures lectures <a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d7849131-0afd-4ae6-ad64-aafb00da36f4" class="external-link">8.2</a>,
<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=50b2e73f-af0a-4715-b627-ab0200ca7bbd" class="external-link">9.1</a>,
[9.2](<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b0299d53-9454-4e33-9086-ab0200db14eeb" class="external-link uri">https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b0299d53-9454-4e33-9086-ab0200db14eeb</a>,
<a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4b6eeb48-ae64-4860-a8c3-ab0200e40ad8" class="external-link">9.3</a>,
and <a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=e998b5dd-bf8e-42da-9f7c-ab1700ca2702" class="external-link">12.1</a>.</li>
</ul>
</div>
<div class="section level2">
<h2 id="modelaveraging">How to use cross-validation for model averaging?<a class="anchor" aria-label="anchor" href="#modelaveraging"></a>
</h2>
<p>If one of the models in the model selection is not clearly the best,
it may be better to average over many models.</p>
<ul>
<li>CV-weights / LOO-weights idea is derived from assuming elpd_loo can
be considered as pseudo marginal likelihood [Geisser+Eddy:1979] and is
related also to Akaike-weights <span class="citation">(<a href="#ref-Burnham+Anderson:2002" role="doc-biblioref">Burnham and
Anderson, 2002</a>)</span>
</li>
<li>LOO-BB-weights improve LOO-weights by taking into account the
uncertainty related to having only a finite sample size to present the
future data distribution <span class="citation">(<a href="#ref-Yao+etal:2018" role="doc-biblioref">Yao <em>et al.</em>,
2018</a>)</span>
</li>
<li>Bayesian stacking optimizes the stacking weights to maximize the
elpd_loo of the combined predictive distribution.</li>
</ul>
<p>Based on the experiments by <span class="citation">Yao <em>et
al.</em> (<a href="#ref-Yao+etal:2018" role="doc-biblioref">2018</a>)</span>, Bayesian stacking has better
performance than LOO-weights and LOO-BB-weights.</p>
</div>
<div class="section level2">
<h2 id="valid">When is cross-validation valid?<a class="anchor" aria-label="anchor" href="#valid"></a>
</h2>
<p>This is about how the data can be divided (see <a href="#parts">What
are the parts of cross-validation?</a>) to estimate expected utility or
loss in certain prediction tasks.</p>
<p>Quite long answer can be found in <a href="https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/" class="external-link">a
blog post</a>.</p>
<p>Some of the points from the blog post are extended below.</p>
<p>LOO and cross-validation in general do not require independence or
conditional independence. Exchangeability is sufficient. Even if we are
using models with conditional independence structure, we don’t require
that the true data generating mechanism has such structure, but due to
exchangeability and the data collection process we can proceed as if
assuming conditional independence. See more in Chapter 5 of BDA3 <span class="citation">(<a href="#ref-BDA3" role="doc-biblioref">Gelman <em>et
al.</em>, 2013</a>)</span>. Cross-validation can also be used when the
model doesn’t have conditional independence structure. In time series,
the observations <span class="math inline">\(y_1,\ldots,y_T\)</span> are
not exchangeable as the index has additional information about the
similarity in time. If we have model <span class="math inline">\(p(y_t
\mid f_t)\)</span>, with latent values <span class="math inline">\(f_t\)</span> then pairs <span class="math inline">\((y_1,f_1),\ldots,(y_T,f_T)\)</span> are
exchangeable (see Chapter 5 of BDA3 <span class="citation">(<a href="#ref-BDA3" role="doc-biblioref">Gelman <em>et al.</em>,
2013</a>)</span>) and we can factorize the likelihood trivially. We
usually can present time series models with explicit latent values <span class="math inline">\(f_t\)</span> , but sometimes we integrate them
analytically out due to computational reasons and then get
non-factorized likelihood for exactly the same model.</p>
<p>If we want to evaluate the goodness of the model part <span class="math inline">\(p(y_t \mid f_t)\)</span>, LOO is fine. If we want
to evaluate the goodness of the time series model part <span class="math inline">\(p(f_1,\ldots,f_T)\)</span>, way may be interested
in 1) goodness for predicting missing data in a middle (think about
audio restoration of recorded music with missing parts, e.g. due to
scratches in the medium) or 2) we may be interested in predicting future
(think about stock market or disease transmission models).</p>
<p>If the likelihood is factorizable (and if it’s not we can make it
factorizable in some cases, see <a href="#timeseries">Can
cross-validation be used for time series?</a>) then this shows in Stan
code as sum of log-likelihood terms. Now it’s possible to define
entities which are sums of those individual log likelihood components.
If the sums are related to exchangeable parts, we may use terms like
leave-one-observation-out (LOO), leave-one-subject-out,
leave-one-time-point-out, etc. If we want additionally restrict the
information flow, for example, in time series we can add constraint that
if <span class="math inline">\(y_t\)</span> is not observed then <span class="math inline">\(y_{t+1},\ldots,y_{T}\)</span> are not observed, we
can use leave-future-out (LFO).</p>
<p>How do we then choose the level of what to leave out in
cross-validation? It depends which level of the model is interesting and
if many levels are interesting then we can do cross-validation at
different levels.</p>
<p>If you want to claim that your scientific hypothesis generalizes
outside the specific observations you have, we need to define what is
scientifically interesting. We are limited by the range of the data
observed (see <span class="citation">Vehtari and Lampinen (<a href="#ref-Vehtari+Lampinen:2002b" role="doc-biblioref">2002</a>)</span>, and <span class="citation">Vehtari and Ojanen (<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">2012</a>)</span>), but if we can’t generalize even
in that range, there is no hope to generalize outside of that range. For
example in brain signal analysis it is useful to know if the time series
model for brain signal is good, but it is scientifically more
interesting to know whether the model learned from a set of brains work
well also for new brains not included in the data used to learn the
posterior (training set in ML terms). Here we are limited to assessing
generalization in the subject population, for example, young university
students. If we can’t generalize from one brain to another even in that
limited population, there is no hop generalizing to brains of
non-young-university-students.</p>
</div>
<div class="section level2">
<h2 id="hierarchical">Can cross-validation be used for hierarchical / multilevel
models?<a class="anchor" aria-label="anchor" href="#hierarchical"></a>
</h2>
<p>The short answer is “Yes”. Hierarchical model is useful, for example,
if there are several subjects and for each subject several trials. As
discussed in <a href="#valid">When is cross-validation valid?</a>, it is
useful to think of the prediction task or generalizability over
different exchangeable entities. We can use different types of
cross-validation to choose the focus. This means that also different
forms of cross-validation are valid for hierarchical models</p>
<ul>
<li>LOO is valid if the focus is in the conditional observation model.
LOO can often already reveal misspecification of the conditional
observation model, and as LOO is often easy to compute, it is fine to
start with LOO to investigate possible model issues.</li>
<li>LOO is also valid if the prediction task is new individuals in the
existing groups.</li>
<li>If the prediction task is to predict for new groups or we are
interested in generalizability for new groups, then leave-one-group-out
(LOGO) is a better choice. Computation of LOGO is often more challenging
as hierarchical models often have group specific parameters, and
removing the all data from one group will change the posterior of those
parameters a lot.</li>
<li>Leave-one-group-out (LOGO) cross-validation usually doesn’t work
with PSIS approach, unless the group specific parameters are integrated
out. This is sometimes possible analytically (<span class="citation">(<a href="#ref-Vehtari+etal:2016:LOO_for_GLVM" role="doc-biblioref"><strong>Vehtari+etal:2016:LOO_for_GLVM?</strong></a>)</span>)
or by quadrature (<span class="citation">(<a href="#ref-Merkle+Furr+Rabe_Hesketh:2019" role="doc-biblioref"><strong>Merkle+Furr+Rabe_Hesketh:2019?</strong></a>)</span>,
<a href="https://avehtari.github.io/modelselection/roaches.html#5_Poisson_model_with_%E2%80%9Crandom_effects%E2%80%9D_and_integrated_LOO" class="external-link">a
Stan code example</a>)</li>
</ul>
<p>See also</p>
<ul>
<li><a href="https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/" class="external-link">A
blog post</a></li>
<li><a href="https://avehtari.github.io/modelselection/" class="external-link">Model selection
videos</a></li>
<li>A case study <a href="https://avehtari.github.io/modelselection/rats_kcv.html" class="external-link">Cross-validation
for hierarchical models</a>
</li>
<li>A case study <a href="https://avehtari.github.io/modelselection/roaches.html" class="external-link">Roaches
cross-validation demo</a> with “randome effects” models</li>
<li><a href="https://link.springer.com/article/10.1007/s11336-019-09679-0" class="external-link">Merkel,
Furr, and Rabe-Hesketh (2019). Bayesian Comparison of Latent Variable
Models: Conditional Versus Marginal Likelihoods. <em>Psychometrika</em>
84:802-829.</a></li>
</ul>
</div>
<div class="section level2">
<h2 id="timeseries">Can cross-validation be used for time series?<a class="anchor" aria-label="anchor" href="#timeseries"></a>
</h2>
<p>The short answer is “Yes” (see, e.g. <span class="citation">Bürkner,
Gabry and Vehtari (<a href="#ref-Burkner+Gabry+Vehtari:LFO-CV:2020" role="doc-biblioref">2020</a>)</span>). If there is a model <span class="math inline">\(p(y_i \mid f_i,\phi)\)</span> and joint time
series prior for <span class="math inline">\((f_1,...,f_T)\)</span> then
<span class="math inline">\(p(y_i \mid f_i,\phi)\)</span> can be
considered independent given <span class="math inline">\(f_i\)</span>
and <span class="math inline">\(\phi\)</span> and likelihood is
factorizable. This is true often and the past values are informative
about future values, but conditionally we know <span class="math inline">\(f_i\)</span>, the past values are not providing
additional information. This should not be confused with that when we
don’t know <span class="math inline">\(f_i\)</span> and integrate over
the posterior of <span class="math inline">\((f_1,...,f_T)\)</span>, as
then <span class="math inline">\(y_i\)</span> are not conditionally
independent given <span class="math inline">\(\phi\)</span>. Also they
are not anymore exchangeable as we have the time ordering telling
additional information. <span class="math inline">\(M\)</span>-step
ahead prediction (see <span class="citation">Bürkner, Gabry and Vehtari
(<a href="#ref-Burkner+Gabry+Vehtari:LFO-CV:2020" role="doc-biblioref">2020</a>)</span>) is more about the usual interest
in predicting future and evaluating the time series model for <span class="math inline">\((f_1,...,f_T)\)</span>, but leave-one-out
cross-validation is valid for assessing conditional part <span class="math inline">\(p(y_i \mid f_i)\)</span>.</p>
<ul>
<li>LOO is valid if the focus is in the conditional observation model.
LOO can often already reveal problems in the model, and as LOO is often
easy to compute, it is fine to start with LOO to investigate possible
model issues.</li>
<li>If the prediction task is to predict for the future, then
leave-future-out (LFO) can be used. Computation of LFO requires a bit
more work, but often a fast approximated computation is sufficient
(<span class="citation">Bürkner, Gabry and Vehtari (<a href="#ref-Burkner+Gabry+Vehtari:LFO-CV:2020" role="doc-biblioref">2020</a>)</span>). In leave-future-out, it is
possible to consider also 1-step-ahead or M-step-ahead predictions
depending on the prediction task.</li>
</ul>
<p>See also</p>
<ul>
<li><a href="https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/" class="external-link">A
blog post</a></li>
<li><a href="https://avehtari.github.io/modelselection/" class="external-link">Model selection
videos</a></li>
<li>Vignette <a href="http://mc-stan.org/loo/articles/loo2-lfo.html" class="external-link">Approximate
leave-future-out cross-validation for Bayesian time series
models</a>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="spatial">Can cross-validation be used for spatial data?<a class="anchor" aria-label="anchor" href="#spatial"></a>
</h2>
<p>The short answer is “Yes”. This is closely related to <a href="#timeseries">the question about time series</a>. If there is a
model <span class="math inline">\(p(y_i \mid f_i,\phi)\)</span> and
joint spatial prior for <span class="math inline">\((f_1,...,f_T)\)</span> then <span class="math inline">\(p(y_i \mid f_i,\phi)\)</span> can be considered
independent given <span class="math inline">\(f_i\)</span> and <span class="math inline">\(\phi\)</span> and likelihood is factorizable. This
is true often and the observations in the nearby regions are correlated,
but conditionally we know <span class="math inline">\(f_i\)</span>, the
nearby observations are not providing additional information. This
should not be confused with that when we don’t know <span class="math inline">\(f_i\)</span> and integrate over the posterior of
<span class="math inline">\((f_1,...,f_T)\)</span>, as then <span class="math inline">\(y_i\)</span> are not conditionally independent
given <span class="math inline">\(\phi\)</span>. Also they are not
anymore exchangeable as we have the spatial ordering telling additional
information.</p>
<ul>
<li>LOO is valid if the focus is in the conditional observation model
<span class="math inline">\(p(y_i \mid f_i)\)</span>. LOO can often
already reveal problems in the model, and as LOO is often easy to
compute, it is fine to start with LOO to investigate possible model
issues.</li>
<li>Various spatially structured cross-validation approaches can be used
when the there is interest in assessing extrapolation to new regions or
otherwise focus the assessment to the spatial model part.</li>
</ul>
<p>See also</p>
<ul>
<li><a href="https://statmodeling.stat.columbia.edu/2018/08/03/loo-cross-validation-approaches-valid/" class="external-link">A
blog post</a></li>
<li><a href="https://avehtari.github.io/modelselection/" class="external-link">Model selection
videos</a></li>
<li>Vignette <a href="http://mc-stan.org/loo/articles/loo2-lfo.html" class="external-link">Approximate
leave-future-out cross-validation for Bayesian time series
models</a>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="otherutility">Can other utility or loss functions be used than log predictive
density?<a class="anchor" aria-label="anchor" href="#otherutility"></a>
</h2>
<p>Short answer is “Yes”. <span class="citation">Vehtari, Gelman and
Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span> state ``Instead of the log
predictive density <span class="math inline">\(\log p(\tilde{y}_i \mid
y)\)</span>, other utility (or loss) functions <span class="math inline">\(u(p(\tilde{y}_i \mid y), \tilde{y})\)</span> could
be used, such as classification error.’’ See also <span class="citation">Vehtari and Ojanen (<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>Vignette for <code>loo</code> package about other utility and loss
functions is work in progress, but there are examples elsewhere:</p>
<ul>
<li>Sections 4.3 and 4.4 in <a href="https://avehtari.github.io/modelselection/diabetes.html" class="external-link">Diabetes
case study</a>, which illustrate how to compute LOO classification
accuracy and LOO calibration plots.</li>
<li>Section 2 in <a href="https://avehtari.github.io/bayes_R2/bayes_R2.html" class="external-link">Online appendix
for Bayesian <span class="math inline">\(R^2\)</span></a> shows how to
compute LOO-<span class="math inline">\(R^2\)</span>, which is just
1-(data variance scaled LOO-MSE).</li>
<li>projpred case studies such as <a href="https://avehtari.github.io/modelselection/collinear.html" class="external-link">collinear</a>,
<a href="https://avehtari.github.io/modelselection/diabetes.html" class="external-link">diabetes</a>,
<a href="https://avehtari.github.io/modelselection/mesquite.html" class="external-link">mesquite</a>,
<a href="https://avehtari.github.io/modelselection/candy.html" class="external-link">candy</a>,
<a href="">winequality-red</a>, <a href="https://avehtari.github.io/modelselection/bodyfat.html" class="external-link">bodyfat</a>,
and <a href="https://mc-stan.org/projpred/articles/quickstart.html" class="external-link">projpred</a>
show LOO-RMSE or LOO classification accuracy in addition of ELPD.</li>
</ul>
<p><code>loo</code> package has functions (<a href="http://mc-stan.org/loo/reference/E_loo.html" class="external-link uri">http://mc-stan.org/loo/reference/E_loo.html</a>) for
computing the necessary expectations. We have plan for adding more on
other utilities and loss functions (see a <a href="https://github.com/stan-dev/loo/issues/135" class="external-link">Github issue</a>).</p>
<p>We recommend log predictive density (log score) for model comparison
in general as it measures the goodness of the whole predictive
distribution (see also <span class="citation">Vehtari and Ojanen (<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">2012</a>)</span>).
We also recommend to use application specific utility and loss functions
which can provide information whether the predictive accuracy is good
enough in practice as compared to application expertise. It is possible
that one model is better than others, but still not useful for practice.
We are happy to get feedback on other utility and loss functions than
log score, RMSE, ACC and <span class="math inline">\(R^2\)</span> that
would be even more application specific.</p>
<p>See also</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Scoring_rule" class="external-link">Scoring rule in
Wikipedia</a></li>
<li>Discussion about proper scoring rules by <a href="https://doi.org/10.1198/016214506000001437" class="external-link">Gneiting and Raftery,
2012</a>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="elpd_interpretation">What is the interpretation of ELPD / elpd_loo / elpd_diff?<a class="anchor" aria-label="anchor" href="#elpd_interpretation"></a>
</h2>
<p>Log densities and log probabilities can be transformed to densities
and probabilities which have intrinsic interpretation, although most are
not well calibrated for the values as they are not used to think in
densities and probabilities and even less in log densities and log
probabilities.</p>
<p>The log probabilities are easier. For example, Guido Biele had a
problem computing <code>elpd_loo</code> with a beta-binomial model for
data with 22 categories. Computed individual <code>elpd_loo</code>
values for observations were around -461. For discrete model with
uniform probability for 22 categories log probabilities would be <span class="math inline">\(\log(1/22)\approx -3.1\)</span>, and thus there
was two orders of magnitude error in log scale. With the fixed code
individual <code>elpd_loo</code> values were about <span class="math inline">\(-2.3&gt;-3.1\)</span>, that is, the model was
beating the uniform distribution.</p>
<p>The log densities are more difficult as they require knowing possible
scaling or transformations of the data. See more in <a href="#differentmodels">Can cross-validation be used to compare
different observation models / response distributions /
likelihoods?</a>.</p>
<p>Although ELPD is good for model comparison as it measures the
goodness of the whole predictive distribution, the difference in ELPD is
even more difficult to interpret without some practice, and thus we
recommend to use also application specific utility or loss functions.
See more in <a href="#otherutility">Can other utility and loss functions
be used than log predictive density?</a>.</p>
<p>As quick rule: If elpd difference (<code>elpd_diff</code> in
<code>loo</code> package) is less than 4, the difference is small <span class="citation">(<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">Sivula, Magnusson and Vehtari, 2020</a>)</span>. If
elpd difference (<code>elpd_diff</code> in loo package) is larger than
4, then compare that difference to standard error of
<code>elpd_diff</code> (provided e.g. by <code>loo</code> package) <span class="citation">(<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">Sivula, Magnusson and Vehtari, 2020</a>)</span>.
The value for deciding what is small or large can be based on connection
to Pseudo-BMA+-weights <span class="citation">(<a href="#ref-Yao+etal:2018" role="doc-biblioref">Yao <em>et al.</em>,
2018</a>)</span>. See also <a href="#se_diff">How to interpret in
Standard error (SE) of elpd difference (elpd_diff)?</a>.</p>
</div>
<div class="section level2">
<h2 id="differentmodels">Can cross-validation be used to compare different observation models
/ response distributions / likelihoods?<a class="anchor" aria-label="anchor" href="#differentmodels"></a>
</h2>
<p>Short answer is “Yes”. First to make the terms more clear, <span class="math inline">\(p(y \mid \theta)\)</span> as a function of <span class="math inline">\(y\)</span> is an observation model and <span class="math inline">\(p(y \mid \theta)\)</span> as a function of <span class="math inline">\(\theta\)</span> is a likelihood. It is better to
ask
<code>Can cross-validation be used to compare different observation models?</code></p>
<ul>
<li>You can compare models given different discrete observation models
and it’s also allowed to have different transformations of <span class="math inline">\(y\)</span> as long as the mapping is bijective
(the probabilities will the stay the same).</li>
<li>You can’t compare densities and probabilities directly. Thus you
can’t compare model given continuous and discrete observation models,
unless you compute probabilities in intervals from the continuous model
(also known as discretising continuous model).</li>
<li>You can compare models given different continuous observation
models, but you have exactly the same <span class="math inline">\(y\)</span> (loo functions in <code>rstanarm</code>
and <code>brms</code> check that the hash of <span class="math inline">\(y\)</span> is the same). If y is transformed, then
the Jacobian of that transformation needs to be included. There is an
example of this in <a href="https://avehtari.github.io/ROS-Examples/Mesquite/mesquite.html" class="external-link">mesquite
case study</a>.</li>
<li>Transformations of variables are briefly discussed in BDA3 p. 21
<span class="citation">(<a href="#ref-BDA3" role="doc-biblioref">Gelman
<em>et al.</em>, 2013</a>)</span> and in <a href="https://mc-stan.org/docs/reference-manual/variable-transforms-chapter.html" class="external-link">Stan
Reference Manual Chapter 10</a>.</li>
</ul>
</div>
<div class="section level2">
<h2 id="is-it-a-problem-to-mix-discrete-and-continuous-data-types">Is it a problem to mix discrete and continuous data types?<a class="anchor" aria-label="anchor" href="#is-it-a-problem-to-mix-discrete-and-continuous-data-types"></a>
</h2>
<p>See also <a href="#differentmodels">Can cross-validation be used to
compare different observation models / response distributions /
likelihoods?</a>.</p>
<p>Likelihood is a function with respect to the parameters and, discrete
observation model can have continuous likelihood function and continuous
observation model can have discrete likelihood function. For example
Stan doesn’t allow discrete parameters unless integrated out by summing,
and thus in Stan you can mix only discrete and continuous observation
models which have continuous likelihood functions.</p>
<p>First we need to think which utility or loss functions make sense for
different data types. Log score can be used for discrete and continuous.
Second we need to be careful with how the continuous data is scaled, as
for example in the case of log score, the scaling affects log-densities
and then log-probabilities and log-densities of arbitrarily scaled data
are not comparable and their contributions would have arbitrary weights
in the combined expected utility or loss.</p>
<p>Scaling of the data doesn’t change probabilities in discrete
observation model. Scaling of the data does change the probability
densities in continuous observation model. People often scale continuous
data before modeling, for example, to have standard deviation of 1. The
same holds for other transformations, e.g. people might compare Poisson
model for discrete counts to normal model for log counts, and then the
results are not comparable. When the probabilities don’t change but
densities change, then the relative weight of components change. So we
need to be careful, either by explicitly discretizing the continuous
distribution to probabilities (see “Can cross-validation be used to
compare different observation models / response distributions /
likelihoods?”) or by keeping the scale such that densities correspond
directly to sensible discretization.</p>
<p>We can also report the performance for discrete and continuous data
separately, by summing the individual pointwise results for discrete and
continuous separately.</p>
</div>
<div class="section level2">
<h2 id="why-sqrtn-in-standard-error-se-of-loo">Why <span class="math inline">\(\sqrt{n}\)</span> in Standard error
(SE) of LOO?<a class="anchor" aria-label="anchor" href="#why-sqrtn-in-standard-error-se-of-loo"></a>
</h2>
<p>As we have only finite number <span class="math inline">\(N\)</span>
of observations which are used by cross-validation as a proxy of the
future data, there is uncertainty in the LOO estimate.</p>
<p>As <span class="math inline">\(\widehat{\mathrm{elpd}}_\mathrm{loo}\)</span> is
defined in Equation (4) by <span class="citation">Vehtari, Gelman and
Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span> as a sum and not as a mean, we
multiply the variance of individual terms by <span class="math inline">\(\sqrt{N}\)</span> instead of dividing by <span class="math inline">\(\sqrt{N}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="se_diff">How to interpret in Standard error (SE) of elpd difference
(elpd_diff)?<a class="anchor" aria-label="anchor" href="#se_diff"></a>
</h2>
<p>SE assumes that normal approximation describes well the uncertainty
related to the expected difference. Due to cross-validation folds not
being independent, SE tends to be underestimated especially if the
number of observations is small or the models are badly misspecified.
The whole normal approximation tends to fail if the models are very
similar or the models are badly misspecified. More about the failure
modes, diagnostics and recommendations are available in a paper by <span class="citation">Sivula, Magnusson and Vehtari (<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">2020</a>)</span>.</p>
<p>tl;dr When the difference (<code>elpd_diff</code>) is larger than 4,
the number of observations is larger than 100 and the model is not badly
misspecified then normal approximation and SE are quite reliable
description of the uncertainty in the difference. Differences smaller
than 4 are small and then the models have very similar predictive
performance and it doesn’t matter if the normal approximation fails or
SE is underestimated <span class="citation">(<a href="#ref-Sivula+etal:2020:loo_uncertainty" role="doc-biblioref">Sivula, Magnusson and Vehtari,
2020</a>)</span>.</p>
</div>
<div class="section level2">
<h2 id="high_khat">What to do if I have many high Pareto <span class="math inline">\(\hat{k}\)</span>’s?<a class="anchor" aria-label="anchor" href="#high_khat"></a>
</h2>
<p>This is about Pareto-<span class="math inline">\(\hat{k}\)</span>
(khat) diagnostic for PSIS-LOO.</p>
<p>The Pareto-<span class="math inline">\(\hat{k}\)</span> is a
diagnostic for Pareto smoothed importance sampling (PSIS) <span class="citation">(<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">Vehtari, Gelman and Gabry, 2017</a>)</span>, which
is used to compute components of <code>elpd_loo</code>. In
importance-sampling LOO (the full posterior distribution is used as the
proposal distribution), the Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic estimates how far an
individual leave-one-out distribution is from the full distribution. If
leaving out an observation changes the posterior too much then
importance sampling is not able to give reliable estimate. If <span class="math inline">\(\hat{k}&lt;0.5\)</span>, then the corresponding
component of <code>elpd_loo</code> is estimated with high accuracy. If
<span class="math inline">\(0.5&lt;\hat{k}&lt;0.7\)</span> the accuracy
is lower, but still OK. If <span class="math inline">\(\hat{k}&gt;0.7\)</span>, then importance sampling
is not able to provide useful estimate for that component/observation.
Pareto-<span class="math inline">\(\hat{k}\)</span> is also useful as a
measure of influence of an observation. Highly influential observations
have high <span class="math inline">\(\hat{k}\)</span> values. Very high
<span class="math inline">\(\hat{k}\)</span> values often indicate model
misspecification, outliers or mistakes in data processing. See Section 6
of <span class="citation">Gabry <em>et al.</em> (<a href="#ref-Gabry+etal:2019:visualization" role="doc-biblioref">2019</a>)</span> for an example.</p>
<p>If there are many high <span class="math inline">\(\hat{k}\)</span>
values, We can gain additional information by looking at
<code>p_loo</code> reported, e.g. by <code>loo</code> package.
<code>p_loo</code> is measure of effective number of parameters (see
more in <a href="#p_loo">What is the interpretation of p_loo?</a>.</p>
<p>If <span class="math inline">\(\hat{k} &gt; 0.7\)</span> then we can
also look at the p_loo estimate for some additional information about
the problem:</p>
<ul>
<li><p>If p_loo <span class="math inline">\(\ll p\)</span> (the total
number of parameters in the model), then the model is likely to be
misspecified. Posterior predictive checks (PPCs) are then likely to also
detect the problem. Try using an overdispersed model, or add more
structural information (nonlinearity, mixture model, etc.).</p></li>
<li><p>If p_loo <span class="math inline">\(&lt; p\)</span> and the
number of parameters <span class="math inline">\(p\)</span> is
relatively large compared to the number of observations (e.g., <span class="math inline">\(p&gt;N/5\)</span>), it is likely that the model is
so flexible or the population prior so weak that it’s difficult to
predict the left out observation (even for the true model). This
happens, for example, in the simulated 8 schools <span class="citation">(<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">Vehtari, Gelman and Gabry, 2017</a>)</span>, random
effect models with a few observations per random effect, and Gaussian
processes and spatial models with short correlation lengths.</p></li>
<li><p>If p_loo <span class="math inline">\(&gt; p\)</span>, then the
model is likely to be badly misspecified. If the number of parameters
<span class="math inline">\(p \ll N\)</span>, then PPCs are also likely
to detect the problem. See for example the <a href="https://avehtari.github.io/modelselection/roaches.html" class="external-link">Roaches
case study</a>. If <span class="math inline">\(p\)</span> is relatively
large compared to the number of observations, say <span class="math inline">\(p&gt;N/5\)</span> (more accurately we should count
number of observations influencing each parameter as in hierarchical
models some groups may have few observations and other groups many), it
is possible that PPCs won’t detect the problem.</p></li>
</ul>
<p>For information see</p>
<ul>
<li>Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian
model evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>. 27(5), 1413–1432. <a href="doi:10.1007/s11222-016-9696-4" class="uri">doi:10.1007/s11222-016-9696-4</a>. <a href="http://link.springer.com/article/10.1007%2Fs11222-016-9696-4" class="external-link">Online</a>.</li>
<li>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019).
Pareto smoothed importance sampling. <a href="http://arxiv.org/abs/1507.02646" class="external-link">arXiv preprint
arXiv:1507.02646</a>.</li>
<li>Video <a href="https://www.youtube.com/watch?v=U_EbJMMVdAU&amp;t=278s" class="external-link">Pareto-$
as practical pre-asymptotic diagnostic of Monte Carlo estimates</a>
(34min)</li>
<li>
<a href="https://www.youtube.com/watch?v=uIojz7lOz9w&amp;list=PLBqnAso5Dy7PCUJbWHO7z3bdeizDdgOhY&amp;index=2" class="external-link">Practical
pre-asymptotic diagnostic of Monte Carlo estimates in Bayesian inference
and machine learning</a> (50min)</li>
</ul>
<p>Moment matching LOO can be used to reduce the number of high Pareto
<span class="math inline">\(k\)</span>’s faster than by refitting all
problematic cases.</p>
<ul>
<li>Paananen, T., Piironen, J., Buerkner, P.-C., Vehtari, A. (2020).
Implicitly adaptive importance sampling. <em>Statistics and
Computing</em>, 31, 16. <a href="https://doi.org/10.1007/s11222-020-09982-2" class="external-link">doi:10.1007/s11222-020-09982-2</a>.</li>
</ul>
</div>
<div class="section level2">
<h2 id="more_parameters">Can I use PSIS-LOO if I have more parameters than observations?<a class="anchor" aria-label="anchor" href="#more_parameters"></a>
</h2>
<p>Yes, but you are likely to have many high Pareto <span class="math inline">\(k\)</span>’s if prior is weak or if there are
parameters which see the information only from one observation each
(e.g. “random” effect models). See an example in <a href="https://avehtari.github.io/modelselection/roaches.html#4_poisson_model_with_%E2%80%9Crandom_effects%E2%80%9D%20vignette" class="external-link">Section
<code>Poisson model with “random effects”</code> in Roaches
cross-validation demo</a>.</p>
</div>
<div class="section level2">
<h2 id="p_loo">What is the interpretation of p_loo?<a class="anchor" aria-label="anchor" href="#p_loo"></a>
</h2>
<p><code>p_loo</code> is called the effective number of parameters and
can be computed as the difference between <code>elpd_loo</code> and the
non-cross-validated log posterior predictive density (Equations (4) and
(3) in <span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>). It is not needed for
<code>elpd_loo</code>, but has diagnostic value. It describes how much
more difficult it is to predict future data than the observed data.
Asymptotically under certain regularity conditions, <code>p_loo</code>
can be interpreted as the effective number of parameters. In well
behaving cases <code>p_loo</code> <span class="math inline">\(&lt;
N\)</span> and <code>p_loo</code> <span class="math inline">\(&lt;
p\)</span>, where <span class="math inline">\(p\)</span> is the total
number of parameters in the model. <code>p_loo</code> <span class="math inline">\(&gt; N\)</span> or <code>p_loo</code> <span class="math inline">\(&gt; p\)</span> indicates that the model has very
weak predictive capability. This can happen even in case of well
specified model (as demonstrated in Figure 1 in <span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>), but may also indicate a severe
model misspecification. See more in “Interpreting p_loo when
Pareto-<span class="math inline">\(\hat{k}\)</span> is large” in <a href="https://mc-stan.org/loo/reference/loo-glossary.html">LOO
Glossary</a>.</p>
</div>
<div class="section level2">
<h2 id="limitations">What are the limitations of the cross-validation?<a class="anchor" aria-label="anchor" href="#limitations"></a>
</h2>
<p>See, for example, <a href="https://doi.org/10.1007/s42113-018-0020-6" class="external-link">Limitations of
“Limitations of Bayesian Leave-one-out Cross-Validation for Model
Selection”</a> and <a href="https://doi.org/10.1007/s42113-018-0019-z" class="external-link">Between the Devil and
the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical
Model Selection</a>.</p>
</div>
<div class="section level2">
<h2 id="LOO_WAIC">How are LOO and WAIC related?<a class="anchor" aria-label="anchor" href="#LOO_WAIC"></a>
</h2>
<p>LOO is an cross-validation approach which can be used to estimate
expected utility and loss functions. If log score is used, LOO can be
used to estimate ELPD and we write elpd_loo.</p>
<p>WAIC is an computational method to estimate ELPD and we could write
elpd_waic. Watanabe used log score, but as WAIC has often be represented
as an alternative to DIC which used -2 * log score, WAIC is sometimes
use to estimate -2*ELPD. See also <a href="#LOOIC">How are LOOIC and
elpd_loo related? Why LOOIC is -2*elpd_loo?</a>.</p>
<p>In theory, the computational method used in WAIC, which corresponds
to a truncated Taylor series approximation of leave-one-out
cross-validation, could be used with other smooth utilities and loss
functions than log score <span class="citation">(<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">Vehtari and Ojanen,
2012</a>)</span>, but we’re not aware of people doing that and we don’t
recommend it as PSIS-LOO has better diagnostics.</p>
<p>All limitations when LOO is valid or sensible hold also for WAIC (see
<a href="#valid">When is cross-validation valid?</a>, <a href="#hierarchical">Can cross-validation be used for
hierarchical/multilevel models?</a>, <a href="#timeseries">Can
cross-validation be used for time series?</a>). Thinking in terms of LOO
cross-validation, it is easier to move to other cross-validation data
division schemes.</p>
<p><span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span> show that PSIS-LOO has usually
smaller error in estimating ELPD than WAIC. The exception is the case
when p_loo <span class="math inline">\(\ll N\)</span>, as then WAIC
tends to have slightly smaller error, but in that case both PSIS-LOO and
WAIC have very small error and it doesn’t matter which computational
approximation is used. On the other hand, for flexible models WAIC fails
more easily, has significant bias and is less easy to diagnose for
failures. WAIC has been included in <code>loo</code> package only for
comparison purposes and to make it easy to replicate the results in
<span class="citation">Vehtari, Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span>.</p>
</div>
<div class="section level2">
<h2 id="LOOIC">How are LOOIC and elpd_loo related? Why LOOIC is -2<span class="math inline">\(*\)</span>elpd_loo?<a class="anchor" aria-label="anchor" href="#LOOIC"></a>
</h2>
<p>Historically, some of the information criterion papers used to use -2
* log score instead of simple log score. The reason for -2 dates to back
in time when maximum likelihood was commonly used, as for Gaussian model
with known variance -2 log score is equal to squared error. Also
asymptotically when using maximum likelihood for estimation and
likelihood ratio test for null hypothesis testing within nested GLMs
there is a connection to Chi^2 distribution.</p>
<p>The historical -2 was carried on to DIC which still was using point
estimates. Watanbe did not use -2 in his WAIC paper. However, when
people started using WAIC instead of DIC, some thought it would be
useful to keep the same scale for comparison. This was what happened
also in BDA3, but later, for example, <span class="citation">Vehtari,
Gelman and Gabry (<a href="#ref-Vehtari+etal:PSIS-LOO:2017" role="doc-biblioref">2017</a>)</span> do not use -2 anymore, as the
above mentioned connections do not hold in general for Bayesian models
in finite case and there is no benefit in multiplying by -2. Future
printings of BDA3 also recommend to not use -2.</p>
<p>If you prefer minimizing losses instead of maximizing utilities,
multiply by -1.</p>
<p>The benefit of not having 2, is that then elpd_loo and p_loo are on
the same scale and comparing models and using p_loo for diagnostics is
easier.</p>
</div>
<div class="section level2">
<h2 id="LOO_and_IC">What is the relationship between AIC, DIC, WAIC and LOO-CV?<a class="anchor" aria-label="anchor" href="#LOO_and_IC"></a>
</h2>
<p>For a longer answer see <span class="citation">Vehtari and Ojanen (<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">2012</a>)</span>.
Akaike’s original idea for <code>an information criterion</code> (AIC)
was to estimate the future predictive performance. There are many other
information criteria, which make different assumptions about the model,
inference, and prediction task <span class="citation">(<a href="#ref-Vehtari+Ojanen:2012" role="doc-biblioref">Vehtari and Ojanen,
2012</a>)</span>. For the most common ones, the differences can be
summarised as</p>
<ul>
<li>AIC: assumes regular and true model, predictions given maximum
likelihood estimate</li>
<li>DIC: assumes regular model, predictions given posterior mean
estimate</li>
<li>WAIC: assumes regular or singular model, predictions given posterior
predictive distribution (ie integrating over the parameters)</li>
<li>LOO-CV: assumes regular or singular model, predictions can be based
on any estimate</li>
<li>Bayesian LOO-CV: assumes regular or singular model, predictions
given posterior predictive distribution (ie integrating over the
parameters)</li>
</ul>
<p>Assuming regular and true model, these are asymptotically (with <span class="math inline">\(N \rightarrow \infty\)</span>) the same. In finite
case and singular models, WAIC and Bayesian LOO are the most sensible
when doing Bayesian modeling. Bayesian LOO has benefits over WAIC as
discussed in <a href="#LOO_WAIC">How are LOO and WAIC related?</a>.</p>
</div>
<div class="section level2">
<h2 id="LOO_and_BF">What is the relationship between LOO-CV and Bayes factor?<a class="anchor" aria-label="anchor" href="#LOO_and_BF"></a>
</h2>
<p>LOO-CV estimates the predictive performance given <span class="math inline">\(N-1\)</span> observations. Bayes factor can be
presented as ratio of predictive performance estimates given <span class="math inline">\(0\)</span> observations. Alternatively Bayes
factor can be interpreted as choosing the maximum a posterior model. -
Bayes factor can be sensible when models are well specified and there is
lot of data compared to the number of parameters, so that maximum a
posteriori estimate is fine and the result is not sensitive to priors -
If there is not a lot of data compared to the number of parameters,
Bayes factor can be much more sensitive to prior choice than LOO-CV - If
the models are not very close to the true model, Bayes factor can be
more unstable than cross-validation <span class="citation">(<a href="#ref-Yao+etal:2018" role="doc-biblioref">Yao <em>et al.</em>,
2018</a>; <a href="#ref-Oelrich+etal:2020:overconfident" role="doc-biblioref">Oelrich <em>et al.</em>, 2020</a>)</span> -
Computation of Bayes factor is more challenging. For example, if
computed from MCMC sample, usually several orders of magnitude bigger
sample sizes are needed for Bayes factor than for LOO-CV - If the models
are well specified, regular, and there is a lot of data compared to the
number of parameters (<span class="math inline">\(n \gg p\)</span>),
then Bayes factor may have smaller variance than LOO-CV. If the models
are nested, instead of Bayes factor, it is also possible to look
directly at the posterior of the interesting parameters (see also 2b in
<a href="#manymodels">Using cross-validation for many models</a>).</p>
</div>
<div class="section level2">
<h2 id="LOO-PIT">What is LOO-PIT<a class="anchor" aria-label="anchor" href="#LOO-PIT"></a>
</h2>
<p>LOO-PIT is a form of posterior predictive checking (PPC). In the
usual PPC, the marginal predictive distribution is compared to all data
(sometimes in groups). See, for example, <a href="https://mc-stan.org/bayesplot/articles/graphical-ppcs.html" class="external-link">a
<code>bayesplot</code> vignette</a>.</p>
<p>If we want to focus on conditional predictive distributions, we often
have only one observation for each conditional predictive distribution.
Using probability integral transformation (PIT, which corresponds to
cumulative probability) we can transform the comparison of conditional
predictive distribution and one observation to values between [0,1]
which jointly have close to uniform distribution if the conditional
predictive distributions are well calibrated. This type of checking can
be more sensitive to reveal such model misspecifications that are not
visible in the usual marginal PPC.</p>
<p>In case of models with a small number of parameters and a large
number of observations, posterior predictive and LOO predictive
distributions are likely to be very similar and we could do conditional
predictive distribution PIT checking without LOO. In case of more
flexible models, smaller number of observations, or highly influential
observations (due to the model flexibility or misspecification) using
LOO predictive distributions can be beneficial (especially when
examining the conditional predictive distributions). See some examples
in the <a href="https://mc-stan.org/bayesplot/reference/PPC-loo.html" class="external-link"><code>loo</code>
package documentation</a>.</p>
<p>LOO-PIT (or posterior predictive PIT) values are in finite case only
close to uniform and not exactly uniform even if the model would include
the true data generating distribution. With small to moderate data sets
the difference can be so small that we can’t see the difference, but
that is why in the above we wrote
<code>close to uniform'' instead of</code>uniform’’. The difference can
be illustrated with a simple normal model. Assume that the data comes
from a normal distribution and consider a model <span class="math inline">\(\mathrm{normal}(\mu, \sigma)\)</span> with classic
uninformative priors. The posterior predictive distribution can then be
computed analytically and is a Student’s <span class="math inline">\(t\)</span> distribution. PIT values from
comparison of a Student’s <span class="math inline">\(t\)</span>
distribution to the normal distributed data are not uniformly
distributed, although with increasing data size, the predictive
distribution will converge towards the true data generating distribution
and the PIT value distribution will converge toward uniform. Thus, in
theory, in case of finite data we can see slight deviation from
uniformity, but that can be assumed to be small compared to what would
be observed in case of bad model misspecification.</p>
<p>At the moment, <a href="https://mc-stan.org/bayesplot/reference/PPC-loo.html" class="external-link"><code>loo</code>
package LOO-PIT functions</a> don’t yet support PIT values for discrete
target distributions, but it is on the todo list of the package
maintainers.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-Burkner+Gabry+Vehtari:LFO-CV:2020" class="csl-entry">
Bürkner, P.-C., Gabry, J. and Vehtari, A. (2020) <span>‘Approximate
leave-future-out cross-validation for <span>Bayesian</span> time series
models’</span>, <em>Journal of Statistical Computation and
Simulation</em>, 90, pp. 2499–2523.
</div>
<div id="ref-Burnham+Anderson:2002" class="csl-entry">
Burnham, K. P. and Anderson, D. R. (2002) <em>Model selection and
multi-model inference: A practical information-theoretic approach</em>.
2nd edn. Springer.
</div>
<div id="ref-Gabry+etal:2019:visualization" class="csl-entry">
Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. and Gelman, A.
(2019) <span>‘Visualization in <span>Bayesian</span> workflow’</span>,
<em>Journal of the Royal Statistical Society: Series A (Statistics in
Society)</em>, 182(2), pp. 389–402.
</div>
<div id="ref-BDA3" class="csl-entry">
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. and
Rubin, D. B. (2013) <em>Bayesian data analysis, third edition</em>. CRC
Press.
</div>
<div id="ref-Gelman+etal:2019:BayesR2" class="csl-entry">
Gelman, A., Goodrich, B., Gabry, J. and Vehtari, A. (2019)
<span>‘R-squared for <span>Bayesian</span> regression models’</span>,
<em>The American Statistician</em>, 73(3), pp. 307–309.
</div>
<div id="ref-Kalliomaki+Vehtari+Lampinen:2005" class="csl-entry">
Kalliomäki, I., Vehtari, A. and Lampinen, J. (2005) <span>‘Shape
analysis of concrete aggregates for statistical quality
modeling’</span>, <em>Machine Vision and Applications</em>, 16(3), pp.
197–201.
</div>
<div id="ref-Navarro:2019:between" class="csl-entry">
Navarro, D. J. (2019) <span>‘Between the devil and the deep blue sea:
Tensions between scientific judgement and statistical model
selection’</span>, <em>Computational Brain &amp; Behavior</em>, 2(1),
pp. 28–34.
</div>
<div id="ref-Oelrich+etal:2020:overconfident" class="csl-entry">
Oelrich, O., Ding, S., Magnusson, M., Vehtari, A. and Villani, M. (2020)
<span>‘When are <span>Bayesian</span> model probabilities
overconfident?’</span>, <em>arXiv preprint arXiv:2003.04026</em>.
</div>
<div id="ref-Paananen+etal:2021:implicit" class="csl-entry">
Paananen, T., Piironen, J., Bürkner, P.-C. and Vehtari, A. (2021)
<span>‘Implicitly adaptive importance sampling.’</span>, <em>Statistics
and Computing</em>, 31(16).
</div>
<div id="ref-Piironen+Vehtari:2017a" class="csl-entry">
Piironen, J. and Vehtari, A. (2017) <span>‘Comparison of
<span>Bayesian</span> predictive methods for model selection’</span>,
<em>Statistics and Computing</em>, 27(3), pp. 711–735. doi: <a href="https://doi.org/10.1007/s11222-016-9649-y" class="external-link">10.1007/s11222-016-9649-y</a>.
</div>
<div id="ref-Sivula+etal:2020:loo_uncertainty" class="csl-entry">
Sivula, T., Magnusson, M. and Vehtari, A. (2020) <span>‘Uncertainty in
<span>B</span>ayesian leave-one-out cross-validation based model
comparison’</span>, <em>arXiv:2008.10296</em>.
</div>
<div id="ref-Tosh+etal:2021:piranha" class="csl-entry">
Tosh, C., Greengard, P., Goodrich, B., Gelman, A., Vehtari, A. and Hsu,
D. (2021) <span>‘The piranha problem: Large effects swimming in a small
pond’</span>, <em>arXiv preprint arXiv:2105.13445</em>.
</div>
<div id="ref-Vehtari+etal:PSIS-LOO:2017" class="csl-entry">
Vehtari, A., Gelman, A. and Gabry, J. (2017) <span>‘Practical
<span>Bayesian</span> model evaluation using leave-one-out
cross-validation and <span>WAIC</span>’</span>, <em>Statistics and
Computing</em>, 27(5), pp. 1413–1432. doi: <a href="https://doi.org/10.1007/s11222-016-9696-4" class="external-link">10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-Vehtari+Lampinen:2002b" class="csl-entry">
Vehtari, A. and Lampinen, J. (2002) <span>‘Bayesian model assessment and
comparison using cross-validation predictive densities’</span>,
<em>Neural Computation</em>, 14(10), pp. 2439–2468.
</div>
<div id="ref-Vehtari+Ojanen:2012" class="csl-entry">
Vehtari, A. and Ojanen, J. (2012) <span>‘A survey of
<span>B</span>ayesian predictive methods for model assessment, selection
and comparison’</span>, <em>Statistics Surveys</em>, 6, pp. 142–228.
doi: <a href="https://doi.org/10.1214/12-SS102" class="external-link">10.1214/12-SS102</a>.
</div>
<div id="ref-Vehtari+etal:PSIS:2019" class="csl-entry">
Vehtari, A., Simpson, D., Gelman, A., Yao, Y. and Gabry, J. (2019)
<span>‘Pareto smoothed importance sampling’</span>, <em>arXiv preprint
arXiv:1507.02646</em>. Available at: <a href="https://arxiv.org/abs/1507.02646v6" class="external-link">https://arxiv.org/abs/1507.02646v6</a>.
</div>
<div id="ref-Yao+etal:2018" class="csl-entry">
Yao, Y., Vehtari, A., Simpson, D. and Gelman, A. (2018) <span>‘Using
stacking to average <span>Bayesian</span> predictive distributions (with
discussion)’</span>, <em>Bayesian Analysis</em>, 13(3), pp. 917–1003.
doi: <a href="https://doi.org/10.1214/17-BA1091" class="external-link">10.1214/17-BA1091</a>.
</div>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="licenses">Licenses<a class="anchor" aria-label="anchor" href="#licenses"></a>
</h2>
<ul>
<li>Text © 2020–2022, Aki Vehtari, licensed under CC-BY-NC 4.0.</li>
</ul>
</div>
<div class="section level2">
<h2 class="unnumbered" id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<p>We thank Jonah Gabry, Ravin Kumar, Martin Modrák, and Erling Rognli
for useful feedback on the draft of FAQ and all who have been asking
questions and discussing answers.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Aki Vehtari, Jonah Gabry, Mans Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, Andrew Gelman.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>

# Continuously ranked probability score

The `crps()` and `scrps()` functions and their `loo_*()` counterparts
can be used to compute the continuously ranked probability score (CRPS)
and scaled CRPS (SCRPS) (as defined by Bolin and Wallin, 2023). CRPS is
a proper scoring rule, and strictly proper when the first moment of the
predictive distribution is finite. Both can be expressed in terms of
samples form the predictive distribution. See, for example, a paper by
Gneiting and Raftery (2007) for a comprehensive discussion on CRPS.

## Usage

``` r
crps(x, ...)

scrps(x, ...)

loo_crps(x, ...)

loo_scrps(x, ...)

# S3 method for class 'matrix'
crps(x, x2, y, ..., permutations = 1)

# S3 method for class 'numeric'
crps(x, x2, y, ..., permutations = 1)

# S3 method for class 'matrix'
loo_crps(
  x,
  x2,
  y,
  log_lik,
  ...,
  permutations = 1,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)

# S3 method for class 'matrix'
scrps(x, x2, y, ..., permutations = 1)

# S3 method for class 'numeric'
scrps(x, x2, y, ..., permutations = 1)

# S3 method for class 'matrix'
loo_scrps(
  x,
  x2,
  y,
  log_lik,
  ...,
  permutations = 1,
  r_eff = 1,
  cores = getOption("mc.cores", 1)
)
```

## Arguments

- x:

  A `S` by `N` matrix (draws by observations), or a vector of length `S`
  when only single observation is provided in `y`.

- ...:

  Passed on to
  [`E_loo()`](https://mc-stan.org/loo/dev/reference/E_loo.md) in the
  `loo_*()` version of these functions.

- x2:

  Independent draws from the same distribution as draws in `x`. Should
  be of the identical dimension.

- y:

  A vector of observations or a single value.

- permutations:

  An integer, with default value of 1, specifying how many times the
  expected value of \|X - X'\| (`|x - x2|`) is computed. The row order
  of `x2` is shuffled as elements `x` and `x2` are typically drawn given
  the same values of parameters. This happens, e.g., when one calls
  [`posterior_predict()`](https://mc-stan.org/rstantools/reference/posterior_predict.html)
  twice for a fitted rstanarm or brms model. Generating more
  permutations is expected to decrease the variance of the computed
  expected value.

- log_lik:

  A log-likelihood matrix the same size as `x`.

- r_eff:

  An optional vector of relative effective sample size estimates
  containing one element per observation. See
  [`psis()`](https://mc-stan.org/loo/dev/reference/psis.md) for details.

- cores:

  The number of cores to use for parallelization of `[psis()]`. See
  [`psis()`](https://mc-stan.org/loo/dev/reference/psis.md) for details.

## Value

A list containing two elements: `estimates` and `pointwise`. The former
reports estimator and standard error and latter the pointwise values.
Following Bolin & Wallin (2023), a larger value is better.

## Details

To compute (S)CRPS, the user needs to provide two sets of draws, `x` and
`x2`, from the predictive distribution. This is due to the fact that
formulas used to compute CRPS involve an expectation of the absolute
difference of `x` and `x2`, both having the same distribution. See the
`permutations` argument, as well as Gneiting and Raftery (2007) for
details.

## References

Bolin, D., & Wallin, J. (2023). Local scale invariance and robustness of
proper scoring rules. Statistical Science, 38(1):140-159.

Gneiting, T., & Raftery, A. E. (2007). Strictly Proper Scoring Rules,
Prediction, and Estimation. Journal of the American Statistical
Association, 102(477), 359â€“378.

## Examples

``` r
# \dontrun{
# An example using rstanarm
library(rstanarm)
data("kidiq")
fit <- stan_glm(kid_score ~ mom_hs + mom_iq, data = kidiq)
#> 
#> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
#> Chain 1: 
#> Chain 1: Gradient evaluation took 2.7e-05 seconds
#> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.
#> Chain 1: Adjust your expectations accordingly!
#> Chain 1: 
#> Chain 1: 
#> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
#> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
#> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
#> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
#> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
#> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
#> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
#> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
#> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
#> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
#> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
#> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
#> Chain 1: 
#> Chain 1:  Elapsed Time: 0.039 seconds (Warm-up)
#> Chain 1:                0.058 seconds (Sampling)
#> Chain 1:                0.097 seconds (Total)
#> Chain 1: 
#> 
#> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
#> Chain 2: 
#> Chain 2: Gradient evaluation took 1.2e-05 seconds
#> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
#> Chain 2: Adjust your expectations accordingly!
#> Chain 2: 
#> Chain 2: 
#> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
#> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
#> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
#> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
#> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
#> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
#> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
#> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
#> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
#> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
#> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
#> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
#> Chain 2: 
#> Chain 2:  Elapsed Time: 0.03 seconds (Warm-up)
#> Chain 2:                0.059 seconds (Sampling)
#> Chain 2:                0.089 seconds (Total)
#> Chain 2: 
#> 
#> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
#> Chain 3: 
#> Chain 3: Gradient evaluation took 1.1e-05 seconds
#> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
#> Chain 3: Adjust your expectations accordingly!
#> Chain 3: 
#> Chain 3: 
#> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
#> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
#> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
#> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
#> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
#> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
#> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
#> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
#> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
#> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
#> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
#> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
#> Chain 3: 
#> Chain 3:  Elapsed Time: 0.032 seconds (Warm-up)
#> Chain 3:                0.06 seconds (Sampling)
#> Chain 3:                0.092 seconds (Total)
#> Chain 3: 
#> 
#> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
#> Chain 4: 
#> Chain 4: Gradient evaluation took 1e-05 seconds
#> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
#> Chain 4: Adjust your expectations accordingly!
#> Chain 4: 
#> Chain 4: 
#> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
#> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
#> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
#> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
#> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
#> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
#> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
#> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
#> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
#> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
#> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
#> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
#> Chain 4: 
#> Chain 4:  Elapsed Time: 0.031 seconds (Warm-up)
#> Chain 4:                0.059 seconds (Sampling)
#> Chain 4:                0.09 seconds (Total)
#> Chain 4: 
ypred1 <- posterior_predict(fit)
ypred2 <- posterior_predict(fit)
crps(ypred1, ypred2, y = fit$y)
#> $estimates
#>   Estimate         SE 
#> -10.219897   0.345828 
#> 
#> $pointwise
#>          1          2          3          4          5          6          7 
#> -24.832608  -9.497974  -7.484053  -4.734042 -21.759348  -7.183888 -30.771228 
#>          8          9         10         11         12         13         14 
#>  -4.768210 -15.674895  -6.507839  -6.374647 -17.725546  -4.207151 -10.606873 
#>         15         16         17         18         19         20         21 
#> -11.609884  -6.759775  -5.388413  -4.342498  -4.420672 -12.485433  -5.675108 
#>         22         23         24         25         26         27         28 
#>  -7.391323  -7.790914  -4.347566 -10.974647  -4.845864  -8.705680  -5.864174 
#>         29         30         31         32         33         34         35 
#>  -4.340388  -4.062520 -15.797182 -30.658642  -6.862946 -12.091037  -5.112402 
#>         36         37         38         39         40         41         42 
#>  -9.534712 -13.107414  -4.689897  -6.022999 -18.267888  -9.979820  -5.443034 
#>         43         44         45         46         47         48         49 
#>  -8.400825  -4.337463 -20.114019  -5.706643 -24.461480  -4.428587 -17.392341 
#>         50         51         52         53         54         55         56 
#>  -4.750338  -6.715633  -5.599897 -19.698814 -14.745689  -5.368889  -8.960394 
#>         57         58         59         60         61         62         63 
#>  -6.213432  -4.947857  -5.266031  -5.621742  -9.251094  -8.769179  -4.334249 
#>         64         65         66         67         68         69         70 
#>  -5.946756 -23.404043 -25.060830 -13.008134  -4.439454  -5.117983  -5.439499 
#>         71         72         73         74         75         76         77 
#>  -5.816072  -5.140044  -4.451072 -14.804729 -11.588105 -16.867868  -4.799698 
#>         78         79         80         81         82         83         84 
#>  -4.763519 -20.329518  -4.723695 -11.888897 -16.055590  -8.785052 -10.105834 
#>         85         86         87         88         89         90         91 
#>  -6.384000  -4.249832 -34.750945  -5.842635  -4.339433  -7.321554  -6.987934 
#>         92         93         94         95         96         97         98 
#> -10.217839  -8.677358  -5.013146 -11.888443  -4.833733  -4.372271  -6.348331 
#>         99        100        101        102        103        104        105 
#> -13.636378 -16.672667  -4.946309 -15.864943 -10.874559  -4.695260  -8.794525 
#>        106        107        108        109        110        111        112 
#>  -4.466705 -12.900131  -7.771510  -5.464964  -8.475276 -29.451592 -13.213411 
#>        113        114        115        116        117        118        119 
#> -26.499370  -7.382165  -5.606401  -5.068504 -25.142035 -29.215471  -5.155325 
#>        120        121        122        123        124        125        126 
#> -15.927693 -11.535546  -5.321693  -4.771637  -4.557614 -19.242695 -13.739164 
#>        127        128        129        130        131        132        133 
#> -10.983371  -5.709663  -4.272685  -8.777254 -23.787892  -8.649466  -6.682855 
#>        134        135        136        137        138        139        140 
#>  -5.613603  -7.093447 -27.667362 -12.649630  -8.167217  -4.542107  -6.693924 
#>        141        142        143        144        145        146        147 
#>  -4.049270  -4.782621  -4.561227  -5.881285  -4.400473  -4.264192  -6.796178 
#>        148        149        150        151        152        153        154 
#> -23.376917 -15.278459 -12.353919  -5.227620 -28.459787  -4.165743  -9.107098 
#>        155        156        157        158        159        160        161 
#>  -8.610086  -4.708598  -7.261103  -7.759657 -10.044562  -4.754171  -5.449334 
#>        162        163        164        165        166        167        168 
#>  -7.349575  -5.708280  -5.725362  -5.144820  -8.330343  -8.578165  -4.092927 
#>        169        170        171        172        173        174        175 
#> -15.912038  -6.395955  -4.841192  -5.584779  -5.744676  -6.776613 -12.378921 
#>        176        177        178        179        180        181        182 
#>  -4.359153  -4.432880  -6.486600  -4.787340  -4.549654  -4.992748  -5.347963 
#>        183        184        185        186        187        188        189 
#>  -4.296344 -13.925748 -19.689421 -14.391662  -5.283480 -20.473847  -4.095086 
#>        190        191        192        193        194        195        196 
#> -10.376654  -8.629100  -4.581903  -7.041547  -6.799590  -7.322993  -4.981426 
#>        197        198        199        200        201        202        203 
#>  -6.772774 -11.200592  -6.640973  -4.629135  -4.795341  -5.005593  -7.810517 
#>        204        205        206        207        208        209        210 
#>  -4.802999  -8.732654  -7.400302  -7.502759  -4.299353 -15.274524  -4.833946 
#>        211        212        213        214        215        216        217 
#>  -7.779151 -12.944115 -39.747915 -19.406502 -11.480787  -6.827946  -6.190020 
#>        218        219        220        221        222        223        224 
#>  -7.857186  -5.166606  -6.194173  -7.795029 -22.973464  -4.962741  -5.176088 
#>        225        226        227        228        229        230        231 
#>  -5.569386  -4.969609  -4.814810  -6.545133 -15.529913  -9.646069  -4.735173 
#>        232        233        234        235        236        237        238 
#>  -4.262584  -8.877559  -7.183162  -6.947994  -4.454690  -7.770403  -4.188412 
#>        239        240        241        242        243        244        245 
#>  -4.542965  -6.086688  -4.424094 -21.614479  -4.219486 -13.588213  -6.637523 
#>        246        247        248        249        250        251        252 
#> -16.007750  -4.339981 -26.254742  -6.992525  -4.740508  -5.568375  -8.659774 
#>        253        254        255        256        257        258        259 
#> -10.914173  -4.821029  -4.271741  -5.779971 -10.458695 -10.969632  -8.332532 
#>        260        261        262        263        264        265        266 
#>  -4.353510  -4.791325  -6.816390  -6.636339 -15.207153  -6.449952  -6.781865 
#>        267        268        269        270        271        272        273 
#>  -6.160805 -21.087572 -19.002166  -8.242650  -4.743159 -27.444163 -42.963092 
#>        274        275        276        277        278        279        280 
#>  -9.981800  -5.726898  -8.192459 -16.599947  -4.338522 -17.716031  -6.212445 
#>        281        282        283        284        285        286        287 
#> -13.931953 -19.362572 -21.413519  -4.113746  -4.234163 -43.128093 -18.025466 
#>        288        289        290        291        292        293        294 
#> -22.472551  -4.327166 -17.965421  -9.502871 -11.028746 -21.920888  -5.722740 
#>        295        296        297        298        299        300        301 
#>  -7.845191 -22.876808  -4.513320 -16.892656  -7.403103 -15.101025  -5.574201 
#>        302        303        304        305        306        307        308 
#> -23.502695  -4.697658  -5.881634  -4.109571  -4.502764 -36.973241  -5.801100 
#>        309        310        311        312        313        314        315 
#>  -4.250955 -28.587011 -12.443797 -29.139971 -10.752458  -4.408391  -4.330023 
#>        316        317        318        319        320        321        322 
#>  -4.214410  -5.437985  -6.909335 -10.075949  -7.032610  -4.880506  -4.439630 
#>        323        324        325        326        327        328        329 
#> -12.723706 -18.271384  -8.436788  -8.928791  -7.244452 -27.198455  -4.447814 
#>        330        331        332        333        334        335        336 
#>  -5.570181  -4.244510 -18.201648 -26.865018 -14.856774  -6.937989 -12.066648 
#>        337        338        339        340        341        342        343 
#> -14.946781 -19.522548  -8.106819  -5.667125 -21.740572  -4.565341 -12.720882 
#>        344        345        346        347        348        349        350 
#>  -3.974364 -16.985312 -16.532402 -29.633357 -16.767958  -5.474551  -6.955609 
#>        351        352        353        354        355        356        357 
#>  -8.691141 -10.189622 -11.537103  -4.161963 -19.944674 -27.262775 -15.695855 
#>        358        359        360        361        362        363        364 
#> -20.508396  -4.172195  -5.374792  -4.648811 -10.170944  -4.453224  -4.748886 
#>        365        366        367        368        369        370        371 
#> -10.970971  -4.521883 -12.834436 -30.305626 -11.811152  -4.583308  -8.922615 
#>        372        373        374        375        376        377        378 
#>  -4.344431  -8.997269  -4.697809 -15.374086 -15.787562 -18.298757  -4.533437 
#>        379        380        381        382        383        384        385 
#>  -4.382424  -6.319694 -14.015959  -9.714661  -7.755039  -9.698901 -11.834002 
#>        386        387        388        389        390        391        392 
#> -13.427477 -11.000098  -4.438268  -6.368325 -10.252900  -9.293052  -4.892745 
#>        393        394        395        396        397        398        399 
#> -19.481020 -20.824090  -4.256602  -5.248329 -20.914437 -24.330966  -4.425286 
#>        400        401        402        403        404        405        406 
#>  -6.675649  -4.448521  -4.444846  -5.814641  -4.559193 -13.869727  -5.390728 
#>        407        408        409        410        411        412        413 
#> -16.339713  -5.348697 -28.205575  -4.292196  -4.515591 -12.726661 -10.108734 
#>        414        415        416        417        418        419        420 
#>  -4.859870  -6.813385  -7.132050  -4.357516 -17.669114  -5.021860 -17.585933 
#>        421        422        423        424        425        426        427 
#> -18.752908  -4.635416 -11.622098  -9.723187 -23.572458  -4.150102  -4.312100 
#>        428        429        430        431        432        433        434 
#>  -7.428655 -16.301398 -12.908698  -6.001770 -19.524187  -4.153528  -8.129198 
#> 
loo_crps(ypred1, ypred2, y = fit$y, log_lik = log_lik(fit))
#> $estimates
#>    Estimate          SE 
#> -10.2976343   0.3502158 
#> 
#> $pointwise
#>   [1] -24.818347  -9.422038  -7.461983  -4.819603 -21.815890  -7.130595
#>   [7] -31.564208  -4.657324 -15.883336  -6.349160  -6.432391 -17.904113
#>  [13]  -4.362669 -10.879007 -11.914873  -6.925017  -5.257308  -4.404165
#>  [19]  -4.586488 -12.666394  -5.608309  -7.242071  -7.830920  -4.415693
#>  [25] -11.065867  -4.709783  -8.953492  -5.941181  -4.147664  -4.077302
#>  [31] -15.793036 -30.657072  -6.975262 -12.234101  -5.230037  -9.481287
#>  [37] -13.233046  -4.915722  -6.118187 -18.256232  -9.773944  -5.429155
#>  [43]  -8.393131  -4.186926 -20.290129  -5.740659 -24.861095  -4.621814
#>  [49] -17.578369  -4.855405  -6.656168  -5.735451 -19.864863 -14.684435
#>  [55]  -5.594150  -8.818524  -6.129134  -5.217341  -5.222929  -5.705199
#>  [61]  -9.256960  -8.722309  -4.507821  -6.219535 -23.403191 -25.259985
#>  [67] -13.124031  -4.448320  -5.063889  -5.376581  -6.118068  -5.045681
#>  [73]  -4.346392 -14.618217 -11.541555 -16.947697  -4.807209  -4.509715
#>  [79] -20.447770  -4.802738 -12.173663 -16.213391  -8.742178 -10.437609
#>  [85]  -6.350138  -4.407243 -35.063199  -5.477867  -4.468189  -7.552003
#>  [91]  -7.209577 -10.152331  -8.845065  -4.786975 -11.960524  -4.804331
#>  [97]  -4.411577  -6.382418 -13.913687 -16.596150  -4.869282 -15.850671
#> [103] -10.929593  -4.763162  -8.662078  -4.273656 -13.172958  -7.861125
#> [109]  -5.596110  -8.591782 -30.451959 -13.554743 -26.723090  -7.695630
#> [115]  -5.503789  -5.241634 -25.288064 -29.698054  -4.978444 -16.031138
#> [121] -11.348527  -5.294892  -4.825359  -4.651897 -19.068360 -13.693992
#> [127] -11.367186  -5.776040  -4.331888  -8.888598 -23.770941  -9.072102
#> [133]  -6.632649  -5.766237  -7.322667 -28.282447 -12.854208  -8.234074
#> [139]  -4.457418  -6.677462  -4.109322  -4.779622  -4.513135  -5.742824
#> [145]  -4.230275  -4.295741  -6.500826 -23.498866 -15.432208 -12.645709
#> [151]  -5.153830 -29.014836  -4.109161  -9.253340  -8.401945  -4.870284
#> [157]  -7.169552  -8.021163  -9.951185  -4.805394  -5.585987  -7.347558
#> [163]  -5.852962  -5.823814  -5.187757  -7.950268  -8.739869  -4.142967
#> [169] -16.130021  -6.592586  -4.761198  -5.504302  -5.712421  -6.756336
#> [175] -12.388149  -4.089791  -4.369140  -6.375400  -4.949080  -4.635577
#> [181]  -5.095038  -5.349561  -4.026976 -14.018068 -19.966953 -14.789051
#> [187]  -5.121961 -20.421257  -4.051622 -10.315829  -8.595238  -4.502327
#> [193]  -7.233960  -6.979352  -7.522677  -5.028059  -6.952241 -11.211828
#> [199]  -6.791078  -4.648653  -5.064459  -5.093267  -7.728478  -4.756190
#> [205]  -8.838256  -7.512762  -7.475512  -4.104173 -15.306885  -4.995432
#> [211]  -7.979530 -12.920003 -40.041951 -19.589240 -11.666233  -6.869234
#> [217]  -6.070463  -7.916474  -5.289014  -6.129267  -7.873053 -23.292152
#> [223]  -4.657037  -5.163047  -5.719120  -4.906539  -4.859326  -6.343195
#> [229] -15.665438  -9.885951  -4.836563  -4.075928  -9.183866  -6.955042
#> [235]  -6.825317  -4.525963  -7.678501  -4.081310  -4.531213  -5.785112
#> [241]  -4.485679 -21.958740  -4.475079 -13.569468  -6.655852 -16.216843
#> [247]  -4.325784 -26.380669  -6.976667  -4.677181  -5.659136  -8.460136
#> [253] -10.786131  -4.896070  -4.003798  -5.767297 -10.575789 -11.153332
#> [259]  -8.729926  -4.331777  -4.934634  -6.649539  -6.834178 -15.322067
#> [265]  -6.554194  -6.683899  -6.216096 -21.415903 -19.245676  -8.369225
#> [271]  -4.522012 -27.586757 -43.421658 -10.063634  -5.806971  -8.026700
#> [277] -16.766966  -4.252190 -17.914444  -6.324771 -14.200497 -19.650373
#> [283] -21.286091  -4.156402  -4.362041 -43.944445 -18.077260 -22.685810
#> [289]  -4.255342 -18.255667  -9.695353 -11.336775 -22.198813  -5.862323
#> [295]  -8.035795 -22.833305  -4.563616 -17.171240  -7.424383 -14.898152
#> [301]  -5.509420 -23.750377  -4.812205  -5.985718  -4.313483  -4.490057
#> [307] -36.777358  -5.637450  -4.199746 -29.371037 -12.459717 -29.252640
#> [313] -11.029418  -4.343983  -4.166621  -4.388568  -5.576455  -6.852322
#> [319] -10.092993  -6.864507  -5.040225  -4.483147 -12.575819 -18.694916
#> [325]  -8.662825  -8.877754  -6.971357 -27.234332  -4.662660  -5.658199
#> [331]  -4.378630 -18.326984 -27.238132 -15.299704  -6.972346 -12.098281
#> [337] -15.239463 -20.001530  -8.201057  -5.781951 -22.178356  -4.438622
#> [343] -12.742583  -4.092238 -17.086342 -16.870452 -30.138228 -16.779874
#> [349]  -5.385299  -7.065001  -8.698734 -10.258097 -11.579909  -4.192561
#> [355] -20.071025 -27.564567 -15.915898 -20.998067  -4.317106  -5.306916
#> [361]  -4.781110 -10.182285  -4.235533  -4.606085 -11.208872  -4.598204
#> [367] -12.714256 -30.708604 -11.983159  -4.590230  -9.054625  -4.389743
#> [373]  -8.902097  -4.768862 -15.562586 -15.860953 -18.435531  -4.423518
#> [379]  -4.452146  -6.568192 -14.270042  -9.902418  -7.916581  -9.672107
#> [385] -12.039543 -13.378377 -10.829800  -4.582418  -6.322141 -10.258800
#> [391]  -9.470946  -5.188656 -20.083757 -21.056899  -4.278845  -5.429531
#> [397] -20.943233 -24.570470  -4.257762  -6.727046  -4.532491  -4.217560
#> [403]  -6.138606  -4.559535 -14.365327  -5.523155 -16.238582  -5.265579
#> [409] -28.190300  -4.203528  -4.437189 -12.990577 -10.125009  -4.823812
#> [415]  -6.752639  -7.193531  -4.404052 -18.005578  -4.999933 -17.924251
#> [421] -18.924142  -4.710594 -11.883241  -9.874364 -23.869978  -4.074721
#> [427]  -4.404031  -7.600876 -16.567730 -13.007297  -5.802904 -19.890509
#> [433]  -4.362580  -7.983001
#> 
# }
```

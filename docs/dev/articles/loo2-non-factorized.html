<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Leave-one-out cross-validation for non-factorized models • loo</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Leave-one-out cross-validation for non-factorized models">
<meta name="robots" content="noindex">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">loo</a>
        <span class="version label label-danger" data-toggle="tooltip" data-placement="bottom" title="In-development version">2.8.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>

  </a>
</li>
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Other Packages

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://mc-stan.org/rstan" class="external-link">rstan</a>
    </li>
    <li>
      <a href="https://mc-stan.org/cmdstanr" class="external-link">cmdstanr</a>
    </li>
    <li>
      <a href="https://mc-stan.org/rstanarm" class="external-link">rstanarm</a>
    </li>
    <li>
      <a href="https://mc-stan.org/bayesplot" class="external-link">bayesplot</a>
    </li>
    <li>
      <a href="https://mc-stan.org/shinystan" class="external-link">shinystan</a>
    </li>
    <li>
      <a href="https://mc-stan.org/projpred" class="external-link">projpred</a>
    </li>
    <li>
      <a href="https://mc-stan.org/rstantools" class="external-link">rstantools</a>
    </li>
    <li>
      <a href="https://mc-stan.org/posterior" class="external-link">posterior</a>
    </li>
  </ul>
</li>
<li>
  <a href="https://mc-stan.org" class="external-link">Stan</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://twitter.com/mcmc_stan" class="external-link">
    <span class="fa fa-twitter"></span>

  </a>
</li>
<li>
  <a href="https://github.com/stan-dev/loo" class="external-link">
    <span class="fa fa-github"></span>

  </a>
</li>
<li>
  <a href="https://discourse.mc-stan.org/" class="external-link">
    <span class="fa fa-users"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Leave-one-out cross-validation for
non-factorized models</h1>
                        <h4 data-toc-skip class="author">Aki Vehtari,
Paul Bürkner and Jonah Gabry</h4>
            
            <h4 data-toc-skip class="date">2025-06-20</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/stan-dev/loo/blob/HEAD/vignettes/loo2-non-factorized.Rmd" class="external-link"><code>vignettes/loo2-non-factorized.Rmd</code></a></small>
      <div class="hidden name"><code>loo2-non-factorized.Rmd</code></div>

    </div>

    
    
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Leave-one-out cross-validation for non-factorized models}
-->
<p><strong>NOTE: We recommend viewing the fully rendered version of this
vignette online at <a href="https://mc-stan.org/loo/articles/" class="uri">https://mc-stan.org/loo/articles/</a></strong></p>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>When computing ELPD-based LOO-CV for a Bayesian model we need to
compute the log leave-one-out predictive densities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\log{p(y_i | y_{-i})}</annotation></semantics></math>
for every response value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><mspace width="0.222em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">y_i, \: i = 1, \ldots, N</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">y_{-i}</annotation></semantics></math>
denotes all response values except observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.
To obtain
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i | y_{-i})</annotation></semantics></math>,
we need to have access to the pointwise likelihood
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i\,|\, y_{-i}, \theta)</annotation></semantics></math>
and integrate over the model parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">
p(y_i\,|\,y_{-i}) =
  \int p(y_i\,|\, y_{-i}, \theta) \, p(\theta\,|\, y_{-i}) \,d \theta
</annotation></semantics></math></p>
<p>Here,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta\,|\, y_{-i})</annotation></semantics></math>
is the leave-one-out posterior distribution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
that is, the posterior distribution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
obtained by fitting the model while holding out the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
observation (we will later show how refitting the model to data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">y_{-i}</annotation></semantics></math>
can be avoided).</p>
<p>If the observation model is formulated directly as the product of the
pointwise observation models, we call it a <em>factorized</em> model. In
this case, the likelihood is also the product of the pointwise
likelihood contributions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i\,|\, y_{-i}, \theta)</annotation></semantics></math>.
To better illustrate possible structures of the observation models, we
formally divide
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
into two parts, observation-specific latent variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>f</mi><mi>N</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f = (f_1, \ldots, f_N)</annotation></semantics></math>
and hyperparameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ψ</mi><annotation encoding="application/x-tex">\psi</annotation></semantics></math>,
so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>f</mi><mi>i</mi></msub><mo>,</mo><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i\,|\, y_{-i}, \theta) = p(y_i\,|\, y_{-i}, f_i, \psi)</annotation></semantics></math>.
Depending on the model, one of the two parts of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
may also be empty. In very simple models, such as linear regression
models, latent variables are not explicitly presented and response
values are conditionally independent given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ψ</mi><annotation encoding="application/x-tex">\psi</annotation></semantics></math>,
so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>f</mi><mi>i</mi></msub><mo>,</mo><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i\,|\, y_{-i}, f_i, \psi) = p(y_i \,|\, \psi)</annotation></semantics></math>.
The full likelihood can then be written in the familiar form</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
p(y \,|\, \psi) = \prod_{i=1}^N p(y_i \,|\, \psi),
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mi>N</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y = (y_1, \ldots, y_N)</annotation></semantics></math>
denotes the vector of all responses. When the likelihood factorizes this
way, the conditional pointwise log-likelihood can be obtained easily by
computing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>ψ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i\,|\, \psi)</annotation></semantics></math>
for each
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
with computational cost
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math>.</p>
<p>Yet, there are several reasons why a <em>non-factorized</em>
observation model may be necessary or preferred. In non-factorized
models, the joint likelihood of the response values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y \,|\, \theta)</annotation></semantics></math>
is not factorized into observation-specific components, but rather given
directly as one joint expression. For some models, an analytic
factorized formulation is simply not available in which case we speak of
a <em>non-factorizable</em> model. Even in models whose observation
model can be factorized in principle, it may still be preferable to use
a non-factorized form for reasons of efficiency and numerical stability
(Bürkner et al. 2020).</p>
<p>Whether a non-factorized model is used by necessity or for efficiency
and stability, it comes at the cost of having no direct access to the
leave-one-out predictive densities and thus to the overall leave-one-out
predictive accuracy. In theory, we can express the observation-specific
likelihoods in terms of the joint likelihood via</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>y</mi><mi>i</mi></msub></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
p(y_i \,|\, y_{i-1}, \theta) = 
  \frac{p(y \,|\, \theta)}{p(y_{-i} \,|\, \theta)} = 
  \frac{p(y \,|\, \theta)}{\int p(y \,|\, \theta) \, d y_i},
</annotation></semantics></math></p>
<p>but the expression on the right-hand side may not always have an
analytical solution. Computing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log p(y_i \,|\, y_{-i},
\theta)</annotation></semantics></math> for non-factorized models is
therefore often impossible, or at least inefficient and numerically
unstable. However, there is a large class of multivariate normal and
Student-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>
models for which there are efficient analytical solutions available.</p>
<p>More details can be found in our paper about LOO-CV for
non-factorized models (Bürkner, Gabry, &amp; Vehtari, 2020), which is
available as a preprint on arXiv (<a href="https://arxiv.org/abs/1810.10559" class="external-link uri">https://arxiv.org/abs/1810.10559</a>).</p>
</div>
<div class="section level2">
<h2 id="loo-cv-for-multivariate-normal-models">LOO-CV for multivariate normal models<a class="anchor" aria-label="anchor" href="#loo-cv-for-multivariate-normal-models"></a>
</h2>
<p>In this vignette, we will focus on non-factorized multivariate normal
models. Based on results of Sundararajan and Keerthi (2001), Bürkner et
al. (2020) show that, for multivariate normal models with coriance
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>,
the LOO predictive mean and standard deviation can be computed as
follows:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>μ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><msub><mi>g</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>σ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msqrt><msubsup><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></msqrt><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  \mu_{\tilde{y},-i} &amp;= y_i-\bar{c}_{ii}^{-1} g_i \nonumber \\
  \sigma_{\tilde{y},-i} &amp;= \sqrt{\bar{c}_{ii}^{-1}},
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding="application/x-tex">g_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\bar{c}_{ii}</annotation></semantics></math>
are
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>g</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>C</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>C</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  g_i &amp;= \left[C^{-1} y\right]_i \nonumber \\
  \bar{c}_{ii} &amp;= \left[C^{-1}\right]_{ii}.
\end{align}</annotation></semantics></math></p>
<p>Using these results, the log predictive density of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
observation is then computed as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><msubsup><mi>σ</mi><mrow><mi>−</mi><mi>i</mi></mrow><mn>2</mn></msubsup><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msubsup><mi>σ</mi><mrow><mi>−</mi><mi>i</mi></mrow><mn>2</mn></msubsup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{-i}
  - \frac{1}{2}\frac{(y_i-\mu_{-i})^2}{\sigma^2_{-i}}.
</annotation></semantics></math></p>
<p>Expressing this same equation in terms of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding="application/x-tex">g_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\bar{c}_{ii}</annotation></semantics></math>,
the log predictive density becomes:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><msub><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mfrac><msubsup><mi>g</mi><mi>i</mi><mn>2</mn></msubsup><msub><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  + \frac{1}{2}\log \bar{c}_{ii}
  - \frac{1}{2}\frac{g_i^2}{\bar{c}_{ii}}.
</annotation></semantics></math> (Note that Vehtari et al. (2016) has a
typo in the corresponding Equation 34.)</p>
<p>From these equations we can now derive a recipe for obtaining the
conditional pointwise log-likelihood for <em>all</em> models that can be
expressed conditionally in terms of a multivariate normal with
invertible covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>.</p>
<div class="section level3">
<h3 id="approximate-loo-cv-using-integrated-importance-sampling">Approximate LOO-CV using integrated importance-sampling<a class="anchor" aria-label="anchor" href="#approximate-loo-cv-using-integrated-importance-sampling"></a>
</h3>
<p>The above LOO equations for multivariate normal models are
conditional on parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
Therefore, to obtain the leave-one-out predictive density
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(y_i \,|\, y_{-i})</annotation></semantics></math>
we need to integrate over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∫</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>θ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
p(y_i\,|\,y_{-i}) =
  \int p(y_i\,|\,y_{-i}, \theta) \, p(\theta\,|\,y_{-i}) \,d\theta.
</annotation></semantics></math></p>
<p>Here,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta\,|\,y_{-i})</annotation></semantics></math>
is the leave-one-out posterior distribution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
that is, the posterior distribution for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
obtained by fitting the model while holding out the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th
observation.</p>
<p>To avoid the cost of sampling from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
leave-one-out posteriors, it is possible to take the posterior draws
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>,</mo><mspace width="0.167em"></mspace><mi>s</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">\theta^{(s)}, \, s=1,\ldots,S</annotation></semantics></math>,
from the posterior
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta\,|\,y)</annotation></semantics></math>,
and then approximate the above integral using integrated importance
sampling (Vehtari et al., 2016, Section 3.6.1):</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mspace width="0.167em"></mspace><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow><mrow><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
 p(y_i\,|\,y_{-i}) \approx
   \frac{ \sum_{s=1}^S p(y_i\,|\,y_{-i},\,\theta^{(s)}) \,w_i^{(s)}}{ \sum_{s=1}^S w_i^{(s)}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">w_i^{(s)}</annotation></semantics></math>
are importance weights. First we compute the raw importance ratios</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>r</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>∝</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mspace width="0.167em"></mspace><msup><mi>θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
  r_i^{(s)} \propto \frac{1}{p(y_i \,|\, y_{-i}, \,\theta^{(s)})},
</annotation></semantics></math></p>
<p>and then stabilize them using Pareto smoothed importance sampling
(PSIS, Vehtari et al, 2019) to obtain the weights
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">w_i^{(s)}</annotation></semantics></math>.
The resulting approximation is referred to as PSIS-LOO (Vehtari et al,
2017).</p>
</div>
<div class="section level3">
<h3 id="exact-loo-cv-with-re-fitting">Exact LOO-CV with re-fitting<a class="anchor" aria-label="anchor" href="#exact-loo-cv-with-re-fitting"></a>
</h3>
<p>In order to validate the approximate LOO procedure, and also in order
to allow exact computations to be made for a small number of
leave-one-out folds for which the Pareto
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
diagnostic (Vehtari et al, 2024) indicates an unstable approximation, we
need to consider how we might to do <em>exact</em> leave-one-out CV for
a non-factorized model. In the case of a Gaussian process that has the
marginalization property, we could just drop the one row and column of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
corresponding to the held out out observation. This does not hold in
general for multivariate normal models, however, and to keep the
original prior we may need to maintain the full covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
even when one of the observations is left out.</p>
<p>The solution is to model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
as a missing observation and estimate it along with all of the other
model parameters. For a conditional multivariate normal model,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log p(y_i\,|\,y_{-i})</annotation></semantics></math>
can be computed as follows. First, we model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
as missing and denote the corresponding parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>y</mi><mi>i</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow></msubsup><annotation encoding="application/x-tex">y_i^{\mathrm{mis}}</annotation></semantics></math>.
Then, we define</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow></msubsup><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mi>N</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
y_{\mathrm{mis}(i)} = (y_1, \ldots, y_{i-1}, y_i^{\mathrm{mis}}, y_{i+1}, \ldots, y_N).
</annotation></semantics></math> to be the same as the full set of
observations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
except replacing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
with the parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>y</mi><mi>i</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow></msubsup><annotation encoding="application/x-tex">y_i^{\mathrm{mis}}</annotation></semantics></math>.</p>
<p>Second, we compute the LOO predictive mean and standard deviations as
above, but replace
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msub><annotation encoding="application/x-tex">y_{\mathrm{mis}(i)}</annotation></semantics></math>
in the computation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\mu_{\tilde{y},-i}</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msub><mo>−</mo><msubsup><mover><mi>c</mi><mo accent="true">‾</mo></mover><mrow><mi>i</mi><mi>i</mi></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><msub><mi>g</mi><mi>i</mi></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mu_{\tilde{y},-i} = y_{{\mathrm{mis}}(i)}-\bar{c}_{ii}^{-1}g_i,
</annotation></semantics></math></p>
<p>where in this case we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><msub><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>C</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><msub><mi>y</mi><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msub><mo stretchy="true" form="postfix">]</mo></mrow><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
g_i = \left[ C^{-1} y_{\mathrm{mis}(i)} \right]_i.
</annotation></semantics></math></p>
<p>The conditional log predictive density is then computed with the
above
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>μ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\mu_{\tilde{y},-i}</annotation></semantics></math>
and the left out observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>log</mo><msubsup><mi>σ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow><mn>2</mn></msubsup><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>μ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msubsup><mi>σ</mi><mrow><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>,</mo><mi>−</mi><mi>i</mi></mrow><mn>2</mn></msubsup></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
  \log p(y_i\,|\,y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{\tilde{y},-i}
  - \frac{1}{2}\frac{(y_i-\mu_{\tilde{y},-i})^2}{\sigma^2_{\tilde{y},-i}}.
</annotation></semantics></math></p>
<p>Finally, the leave-one-out predictive distribution can then be
estimated as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mi>−</mi><mi>i</mi></mrow></msub><mo>,</mo><msubsup><mi>θ</mi><mrow><mi>−</mi><mi>i</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
 p(y_i\,|\,y_{-i}) \approx \sum_{s=1}^S p(y_i\,|\,y_{-i}, \theta_{-i}^{(s)}),
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>θ</mi><mrow><mi>−</mi><mi>i</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_{-i}^{(s)}</annotation></semantics></math>
are draws from the posterior distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mspace width="0.167em"></mspace><mo stretchy="false" form="prefix">|</mo><mspace width="0.167em"></mspace><msub><mi>y</mi><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\theta\,|\,y_{\mathrm{mis}(i)})</annotation></semantics></math>.</p>
</div>
</div>
<div class="section level2">
<h2 id="lagged-sar-models">Lagged SAR models<a class="anchor" aria-label="anchor" href="#lagged-sar-models"></a>
</h2>
<p>A common non-factorized multivariate normal model is the
simultaneously autoregressive (SAR) model, which is frequently used for
spatially correlated data. The lagged SAR model is defined as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>ρ</mi><mi>W</mi><mi>y</mi><mo>+</mo><mi>η</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">
y = \rho Wy + \eta + \epsilon
</annotation></semantics></math> or equivalently
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>y</mi><mo>=</mo><mi>η</mi><mo>+</mo><mi>ϵ</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
(I - \rho W)y = \eta + \epsilon,
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>
is the spatial correlation parameter and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
is a user-defined weight matrix. The matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
has entries
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w_{ii} = 0</annotation></semantics></math>
along the diagonal and the off-diagonal entries
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math>
are larger when areas
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
are closer to each other. In a linear model, the predictor term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>
is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi><mo>=</mo><mi>X</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">\eta = X \beta</annotation></semantics></math>
with design matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and regression coefficients
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>.
However, since the above equation holds for arbitrary
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>,
these results are not restricted to linear models.</p>
<p>If we have
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mi mathvariant="normal">N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mspace width="0.167em"></mspace><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\epsilon \sim {\mathrm N}(0, \,\sigma^2 I)</annotation></semantics></math>,
it follows that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>y</mi><mo>∼</mo><mi mathvariant="normal">N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>η</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
(I - \rho W)y \sim {\mathrm N}(\eta, \sigma^2 I),
</annotation></semantics></math> which corresponds to the following log
PDF coded in <strong>Stan</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="sc">/</span><span class="er">**</span> </span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a> <span class="er">*</span> Normal log<span class="sc">-</span>pdf <span class="cf">for</span> spatially lagged responses</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a> <span class="sc">*</span> </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a> <span class="er">*</span> <span class="er">@</span>param y Vector of response values.</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param mu Mean parameter vector.</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param sigma Positive scalar residual standard deviation.</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param rho Positive scalar autoregressive parameter.</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param W Spatial weight matrix.</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a> <span class="sc">*</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a> <span class="er">*</span> <span class="er">@</span>return A scalar to be added to the log posterior.</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a> <span class="sc">*</span><span class="er">/</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>real <span class="fu">normal_lagsar_lpdf</span>(vector y, vector mu, real sigma, </span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>                        real rho, matrix W) {</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>  int N <span class="ot">=</span> <span class="fu">rows</span>(y);</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>  real inv_sigma2 <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">square</span>(sigma);</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>  matrix[N, N] W_tilde <span class="ot">=</span> <span class="sc">-</span>rho <span class="sc">*</span> W;</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>  vector[N] half_pred;</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>  </span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) W_tilde[n,n] <span class="sc">+</span><span class="er">=</span> <span class="dv">1</span>;</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>  </span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>  half_pred <span class="ot">=</span> W_tilde <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mdivide_left</span>(W_tilde, mu));</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>  </span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>  return <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log_determinant</span>(<span class="fu">crossprod</span>(W_tilde) <span class="sc">*</span> inv_sigma2) <span class="sc">-</span></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>         <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">dot_self</span>(half_pred) <span class="sc">*</span> inv_sigma2;</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>}</span></code></pre></div>
<p>For the purpose of computing LOO-CV, it makes sense to rewrite the
SAR model in slightly different form. Conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>,
if we write</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>y</mi><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>η</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi mathvariant="normal">N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mi>T</mi></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
y-(I-\rho W)^{-1}\eta &amp;\sim {\mathrm N}(0, \sigma^2(I-\rho W)^{-1}(I-\rho W)^{-T}),
\end{align}</annotation></semantics></math> or more compactly, with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>W</mi><mo accent="true">̃</mo></mover><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>ρ</mi><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widetilde{W}=(I-\rho W)</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>y</mi><mo>−</mo><msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>η</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mi mathvariant="normal">N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mi>T</mi></msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
y-\widetilde{W}^{-1}\eta &amp;\sim {\mathrm N}(0, \sigma^2(\widetilde{W}^{T}\widetilde{W})^{-1}),
\end{align}</annotation></semantics></math></p>
<p>then this has the same form as the zero mean Gaussian process from
above. Accordingly, we can compute the leave-one-out predictive
densities with the equations from Sundararajan and Keerthi (2001),
replacing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>η</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(y-\widetilde{W}^{-1}\eta)</annotation></semantics></math>
and taking the covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
to be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mi>T</mi></msup><mover><mi>W</mi><mo accent="true">̃</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\sigma^2(\widetilde{W}^{T}\widetilde{W})^{-1}</annotation></semantics></math>.</p>
<div class="section level3">
<h3 id="case-study-neighborhood-crime-in-columbus-ohio">Case Study: Neighborhood Crime in Columbus, Ohio<a class="anchor" aria-label="anchor" href="#case-study-neighborhood-crime-in-columbus-ohio"></a>
</h3>
<p>In order to demonstrate how to carry out the computations implied by
these equations, we will first fit a lagged SAR model to data on crime
in 49 different neighborhoods of Columbus, Ohio during the year 1980.
The data was originally described in Aneslin (1988) and ships with the
<strong>spdep</strong> R package.</p>
<p>In addition to the <strong>loo</strong> package, for this analysis we
will use the <strong>brms</strong> interface to Stan to generate a Stan
program and fit the model, and also the <strong>bayesplot</strong> and
<strong>ggplot2</strong> packages for plotting.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://mc-stan.org/loo/">"loo"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/paul-buerkner/brms" class="external-link">"brms"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://mc-stan.org/bayesplot/" class="external-link">"bayesplot"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org" class="external-link">"ggplot2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://mc-stan.org/bayesplot/reference/bayesplot-colors.html" class="external-link">color_scheme_set</a></span><span class="op">(</span><span class="st">"brightblue"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme_get.html" class="external-link">theme_set</a></span><span class="op">(</span><span class="fu"><a href="https://mc-stan.org/bayesplot/reference/theme_default.html" class="external-link">theme_default</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">SEED</span> <span class="op">&lt;-</span> <span class="fl">10001</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="va">SEED</span><span class="op">)</span> <span class="co"># only sets seed for R (seed for Stan set later)</span></span>
<span></span>
<span><span class="co"># loads COL.OLD data frame and COL.nb neighbor list</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">oldcol</span>, package <span class="op">=</span> <span class="st">"spdep"</span><span class="op">)</span> </span></code></pre></div>
<p>The three variables in the data set relevant to this example are:</p>
<ul>
<li>
<code>CRIME</code>: the number of residential burglaries and vehicle
thefts per thousand households in the neighbood</li>
<li>
<code>HOVAL</code>: housing value in units of $1000 USD</li>
<li>
<code>INC</code>: household income in units of $1000 USD</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">COL.OLD</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"CRIME"</span>, <span class="st">"HOVAL"</span>, <span class="st">"INC"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>We will also use the object <code>COL.nb</code>, which is a list
containing information about which neighborhoods border each other. From
this list we will be able to construct the weight matrix to used to help
account for the spatial dependency among the observations.</p>
<div class="section level4">
<h4 id="fit-lagged-sar-model">Fit lagged SAR model<a class="anchor" aria-label="anchor" href="#fit-lagged-sar-model"></a>
</h4>
<p>A model predicting <code>CRIME</code> from <code>INC</code> and
<code>HOVAL</code>, while accounting for the spatial dependency via an
SAR structure, can be specified in <strong>brms</strong> as follows.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html" class="external-link">brm</a></span><span class="op">(</span></span>
<span>  <span class="va">CRIME</span> <span class="op">~</span> <span class="va">INC</span> <span class="op">+</span> <span class="va">HOVAL</span> <span class="op">+</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/sar.html" class="external-link">sar</a></span><span class="op">(</span><span class="va">COL.nb</span>, type <span class="op">=</span> <span class="st">"lag"</span><span class="op">)</span>, </span>
<span>  data <span class="op">=</span> <span class="va">COL.OLD</span>,</span>
<span>  data2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>COL.nb <span class="op">=</span> <span class="va">COL.nb</span><span class="op">)</span>,</span>
<span>  chains <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  seed <span class="op">=</span> <span class="va">SEED</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The code above fits the model in <strong>Stan</strong> using a log
PDF equivalent to the <code>normal_lagsar_lpdf</code> function we
defined above. In the summary output below we see that both higher
income and higher housing value predict lower crime rates in the
neighborhood. Moreover, there seems to be substantial spatial
correlation between adjacent neighborhoods, as indicated by the
posterior distribution of the <code>lagsar</code> parameter.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lagsar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">fit</span>, pars <span class="op">=</span> <span class="st">"lagsar"</span><span class="op">)</span></span>
<span><span class="va">estimates</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/quantile.html" class="external-link">quantile</a></span><span class="op">(</span><span class="va">lagsar</span>, probs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://mc-stan.org/bayesplot/reference/MCMC-distributions.html" class="external-link">mcmc_hist</a></span><span class="op">(</span><span class="va">lagsar</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://mc-stan.org/bayesplot/reference/bayesplot-helpers.html" class="external-link">vline_at</a></span><span class="op">(</span><span class="va">estimates</span>, linetype <span class="op">=</span> <span class="fl">2</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ggtitle</a></span><span class="op">(</span><span class="st">"lagsar: posterior median and 50% central interval"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="approximate-loo-cv">Approximate LOO-CV<a class="anchor" aria-label="anchor" href="#approximate-loo-cv"></a>
</h4>
<p>After fitting the model, the next step is to compute the pointwise
log-likelihood values needed for approximate LOO-CV. To do this we will
use the recipe laid out in the previous sections.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">fit</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">CRIME</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="va">S</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span></span>
<span><span class="va">loglik</span> <span class="op">&lt;-</span> <span class="va">yloo</span> <span class="op">&lt;-</span> <span class="va">sdloo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span>nrow <span class="op">=</span> <span class="va">S</span>, ncol <span class="op">=</span> <span class="va">N</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">s</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">S</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">posterior</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span></span>
<span>  <span class="va">eta</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">$</span><span class="va">b_Intercept</span> <span class="op">+</span> <span class="va">p</span><span class="op">$</span><span class="va">b_INC</span> <span class="op">*</span> <span class="va">fit</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">INC</span> <span class="op">+</span> <span class="va">p</span><span class="op">$</span><span class="va">b_HOVAL</span> <span class="op">*</span> <span class="va">fit</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">HOVAL</span></span>
<span>  <span class="va">W_tilde</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">-</span> <span class="va">p</span><span class="op">$</span><span class="va">lagsar</span> <span class="op">*</span> <span class="fu">spdep</span><span class="fu">::</span><span class="fu">nb2mat</span><span class="op">(</span><span class="va">COL.nb</span><span class="op">)</span></span>
<span>  <span class="va">Cinv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">W_tilde</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">W_tilde</span> <span class="op">/</span> <span class="va">p</span><span class="op">$</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span>  <span class="va">g</span> <span class="op">&lt;-</span> <span class="va">Cinv</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">W_tilde</span>, <span class="va">eta</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">cbar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">Cinv</span><span class="op">)</span></span>
<span>  <span class="va">yloo</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">-</span> <span class="va">g</span> <span class="op">/</span> <span class="va">cbar</span></span>
<span>  <span class="va">sdloo</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">cbar</span><span class="op">)</span></span>
<span>  <span class="va">loglik</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">yloo</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span>, <span class="va">sdloo</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># use loo for psis smoothing</span></span>
<span><span class="va">log_ratios</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="va">loglik</span></span>
<span><span class="va">psis_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/psis.html">psis</a></span><span class="op">(</span><span class="va">log_ratios</span><span class="op">)</span></span></code></pre></div>
<p>The quality of the PSIS-LOO approximation can be investigated
graphically by plotting the Pareto-k estimate for each observation. The
approximation is robust up to values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.7</mn><annotation encoding="application/x-tex">0.7</annotation></semantics></math>
(Vehtari et al, 2017, 2024). In the plot below, we see that the fourth
observation is problematic and so may reduce the accuracy of the LOO-CV
approximation.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">psis_result</span>, label_points <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>We can also check that the conditional leave-one-out predictive
distribution equations work correctly, for instance, using the last
posterior draw:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">yloo_sub</span> <span class="op">&lt;-</span> <span class="va">yloo</span><span class="op">[</span><span class="va">S</span>, <span class="op">]</span></span>
<span><span class="va">sdloo_sub</span> <span class="op">&lt;-</span> <span class="va">sdloo</span><span class="op">[</span><span class="va">S</span>, <span class="op">]</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  y <span class="op">=</span> <span class="va">y</span>, </span>
<span>  yloo <span class="op">=</span> <span class="va">yloo_sub</span>,</span>
<span>  ymin <span class="op">=</span> <span class="va">yloo_sub</span> <span class="op">-</span> <span class="va">sdloo_sub</span> <span class="op">*</span> <span class="fl">2</span>,</span>
<span>  ymax <span class="op">=</span> <span class="va">yloo_sub</span> <span class="op">+</span> <span class="va">sdloo_sub</span> <span class="op">*</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span>data<span class="op">=</span><span class="va">df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">y</span>, y <span class="op">=</span> <span class="va">yloo</span>, ymin <span class="op">=</span> <span class="va">ymin</span>, ymax <span class="op">=</span> <span class="va">ymax</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_linerange.html" class="external-link">geom_errorbar</a></span><span class="op">(</span></span>
<span>    width <span class="op">=</span> <span class="fl">1</span>, </span>
<span>    color <span class="op">=</span> <span class="st">"skyblue3"</span>, </span>
<span>    position <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_jitter.html" class="external-link">position_jitter</a></span><span class="op">(</span>width <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_abline</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"gray30"</span>, size <span class="op">=</span> <span class="fl">1.2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Finally, we use PSIS-LOO to approximate the expected log predictive
density (ELPD) for new data, which we will validate using exact LOO-CV
in the upcoming section.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">psis_loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loo.html">loo</a></span><span class="op">(</span><span class="va">loglik</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="exact-loo-cv">Exact LOO-CV<a class="anchor" aria-label="anchor" href="#exact-loo-cv"></a>
</h4>
<p>Exact LOO-CV for the above example is somewhat more involved, as we
need to re-fit the model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
times and each time model the held-out data point as a parameter. First,
we create an empty dummy model that we will update below as we loop over
the observations.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># see help("mi", "brms") for details on the mi() usage</span></span>
<span><span class="va">fit_dummy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/brm.html" class="external-link">brm</a></span><span class="op">(</span></span>
<span>  <span class="va">CRIME</span> <span class="op">|</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/mi.html" class="external-link">mi</a></span><span class="op">(</span><span class="op">)</span> <span class="op">~</span> <span class="va">INC</span> <span class="op">+</span> <span class="va">HOVAL</span> <span class="op">+</span> <span class="fu"><a href="https://paulbuerkner.com/brms/reference/sar.html" class="external-link">sar</a></span><span class="op">(</span><span class="va">COL.nb</span>, type <span class="op">=</span> <span class="st">"lag"</span><span class="op">)</span>, </span>
<span>  data <span class="op">=</span> <span class="va">COL.OLD</span>,</span>
<span>  data2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>COL.nb <span class="op">=</span> <span class="va">COL.nb</span><span class="op">)</span>,</span>
<span>  chains <span class="op">=</span> <span class="fl">0</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Next, we fit the model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
times, each time leaving out a single observation and then computing the
log predictive density for that observation. For obvious reasons, this
takes much longer than the approximation we computed above, but it is
necessary in order to validate the approximate LOO-CV method. Thanks to
the PSIS-LOO approximation, in general doing these slow exact
computations can be avoided.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">S</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="va">N</span><span class="op">)</span></span>
<span><span class="va">loglik</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span>nrow <span class="op">=</span> <span class="va">S</span>, ncol <span class="op">=</span> <span class="va">N</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">dat_mi</span> <span class="op">&lt;-</span> <span class="va">COL.OLD</span></span>
<span>  <span class="va">dat_mi</span><span class="op">$</span><span class="va">CRIME</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span>
<span>  <span class="va">fit_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">fit_dummy</span>, newdata <span class="op">=</span> <span class="va">dat_mi</span>, </span>
<span>                  <span class="co"># just for vignette</span></span>
<span>                  chains <span class="op">=</span> <span class="fl">1</span>, iter <span class="op">=</span> <span class="va">S</span> <span class="op">*</span> <span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="va">posterior</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">fit_i</span><span class="op">)</span></span>
<span>  <span class="va">yloo</span> <span class="op">&lt;-</span> <span class="va">sdloo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">S</span><span class="op">)</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">s</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">S</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">posterior</span><span class="op">[</span><span class="va">s</span>, <span class="op">]</span></span>
<span>    <span class="va">y_miss_i</span> <span class="op">&lt;-</span> <span class="va">y</span></span>
<span>    <span class="va">y_miss_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">$</span><span class="va">Ymi</span></span>
<span>    <span class="va">eta</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">$</span><span class="va">b_Intercept</span> <span class="op">+</span> <span class="va">p</span><span class="op">$</span><span class="va">b_INC</span> <span class="op">*</span> <span class="va">fit_i</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">INC</span> <span class="op">+</span> <span class="va">p</span><span class="op">$</span><span class="va">b_HOVAL</span> <span class="op">*</span> <span class="va">fit_i</span><span class="op">$</span><span class="va">data</span><span class="op">$</span><span class="va">HOVAL</span></span>
<span>    <span class="va">W_tilde</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">-</span> <span class="va">p</span><span class="op">$</span><span class="va">lagsar</span> <span class="op">*</span> <span class="fu">spdep</span><span class="fu">::</span><span class="fu">nb2mat</span><span class="op">(</span><span class="va">COL.nb</span><span class="op">)</span></span>
<span>    <span class="va">Cinv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">W_tilde</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">W_tilde</span> <span class="op">/</span> <span class="va">p</span><span class="op">$</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span>    <span class="va">g</span> <span class="op">&lt;-</span> <span class="va">Cinv</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="op">(</span><span class="va">y_miss_i</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/solve.html" class="external-link">solve</a></span><span class="op">(</span><span class="va">W_tilde</span>, <span class="va">eta</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">cbar</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="va">Cinv</span><span class="op">)</span>;</span>
<span>    <span class="va">yloo</span><span class="op">[</span><span class="va">s</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">y_miss_i</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">g</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">/</span> <span class="va">cbar</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">sdloo</span><span class="op">[</span><span class="va">s</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="va">cbar</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="va">loglik</span><span class="op">[</span><span class="va">s</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">dnorm</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">yloo</span><span class="op">[</span><span class="va">s</span><span class="op">]</span>, <span class="va">sdloo</span><span class="op">[</span><span class="va">s</span><span class="op">]</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">ypred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">S</span>, <span class="va">yloo</span>, <span class="va">sdloo</span><span class="op">)</span></span>
<span>  <span class="va">res</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">posterior</span><span class="op">$</span><span class="va">Ymi</span>, <span class="va">ypred</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">res</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">type</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pp"</span>, <span class="st">"loo"</span><span class="op">)</span>, each <span class="op">=</span> <span class="va">S</span><span class="op">)</span></span>
<span>  <span class="va">res</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">obs</span> <span class="op">&lt;-</span> <span class="va">i</span></span>
<span><span class="op">}</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">rbind</span>, <span class="va">res</span><span class="op">)</span></span></code></pre></div>
<p>A first step in the validation of the pointwise predictive density is
to compare the distribution of the implied response values for the
left-out observation to the distribution of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>y</mi><mi>i</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi></mrow></msubsup><annotation encoding="application/x-tex">y_i^{\mathrm{mis}}</annotation></semantics></math>
posterior-predictive values estimated as part of the model. If the
pointwise predictive density is correct, the two distributions should
match very closely (up to sampling error). In the plot below, we overlay
these two distributions for the first four observations and see that
they match very closely (as is the case for all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>49</mn><annotation encoding="application/x-tex">49</annotation></semantics></math>
observations of in this example).</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">res_sub</span> <span class="op">&lt;-</span> <span class="va">res</span><span class="op">[</span><span class="va">res</span><span class="op">$</span><span class="va">obs</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span>, <span class="op">]</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">res_sub</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">y</span>, fill <span class="op">=</span> <span class="va">type</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html" class="external-link">geom_density</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_wrap.html" class="external-link">facet_wrap</a></span><span class="op">(</span><span class="st">"obs"</span>, scales <span class="op">=</span> <span class="st">"fixed"</span>, ncol <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<p>In the final step, we compute the ELPD based on the exact LOO-CV and
compare it to the approximate PSIS-LOO result computed earlier.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">log_mean_exp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># more stable than log(mean(exp(x)))</span></span>
<span>  <span class="va">max_x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">max_x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">max_x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">exact_elpds</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">loglik</span>, <span class="fl">2</span>, <span class="va">log_mean_exp</span><span class="op">)</span></span>
<span><span class="va">exact_elpd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">exact_elpds</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">exact_elpd</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>The results of the approximate and exact LOO-CV are similar but not
as close as we would expect if there were no problematic observations.
We can investigate this issue more closely by plotting the approximate
against the exact pointwise ELPD values.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  approx_elpd <span class="op">=</span> <span class="va">psis_loo</span><span class="op">$</span><span class="va">pointwise</span><span class="op">[</span>, <span class="st">"elpd_loo"</span><span class="op">]</span>,</span>
<span>  exact_elpd <span class="op">=</span> <span class="va">exact_elpds</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">approx_elpd</span>, y <span class="op">=</span> <span class="va">exact_elpd</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html" class="external-link">geom_abline</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"gray30"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="fl">4</span>, <span class="op">]</span>, size <span class="op">=</span> <span class="fl">3</span>, color <span class="op">=</span> <span class="st">"red3"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">"Approximate elpds"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">"Exact elpds"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_fixed.html" class="external-link">coord_fixed</a></span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">16</span>, <span class="op">-</span><span class="fl">3</span><span class="op">)</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">16</span>, <span class="op">-</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>In the plot above the fourth data point —the observation flagged as
problematic by the PSIS-LOO approximation— is colored in red and is the
clear outlier. Otherwise, the correspondence between the exact and
approximate values is strong. In fact, summing over the pointwise ELPD
values and leaving out the fourth observation yields practically
equivalent results for approximate and exact LOO-CV:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">without_pt_4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  approx <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">psis_loo</span><span class="op">$</span><span class="va">pointwise</span><span class="op">[</span><span class="op">-</span><span class="fl">4</span>, <span class="st">"elpd_loo"</span><span class="op">]</span><span class="op">)</span>,</span>
<span>  exact <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">exact_elpds</span><span class="op">[</span><span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>  </span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">without_pt_4</span>, <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<p>From this we can conclude that the difference we found when including
<em>all</em> observations does not indicate a bug in our implementation
of the approximate LOO-CV but rather a violation of its assumptions.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="working-with-stan-directly">Working with Stan directly<a class="anchor" aria-label="anchor" href="#working-with-stan-directly"></a>
</h2>
<p>So far, we have specified the models in brms and only used Stan
implicitely behind the scenes. This allowed us to focus on the primary
purpose of validating approximate LOO-CV for non-factorized models.
However, we would also like to show how everything can be set up in Stan
directly. The Stan code brms generates is human readable and so we can
use it to learn some of the essential aspects of Stan and the particular
model we are implementing. The Stan program below is a slightly modified
version of the code extracted via <code>stancode(fit_dummy)</code>:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="sc">/</span><span class="er">/</span> generated with brms <span class="dv">2</span>.<span class="fl">2.0</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>functions {</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="sc">/</span><span class="er">**</span> </span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a> <span class="er">*</span> Normal log<span class="sc">-</span>pdf <span class="cf">for</span> spatially lagged responses</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a> <span class="sc">*</span> </span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a> <span class="er">*</span> <span class="er">@</span>param y Vector of response values.</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param mu Mean parameter vector.</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param sigma Positive scalar residual standard deviation.</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param rho Positive scalar autoregressive parameter.</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a> <span class="sc">*</span> <span class="er">@</span>param W Spatial weight matrix.</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a> <span class="sc">*</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a> <span class="er">*</span> <span class="er">@</span>return A scalar to be added to the log posterior.</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a> <span class="sc">*</span><span class="er">/</span></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>  real <span class="fu">normal_lagsar_lpdf</span>(vector y, vector mu, real sigma,</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>                          real rho, matrix W) {</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>    int N <span class="ot">=</span> <span class="fu">rows</span>(y);</span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a>    real inv_sigma2 <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">square</span>(sigma);</span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>    matrix[N, N] W_tilde <span class="ot">=</span> <span class="sc">-</span>rho <span class="sc">*</span> W;</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a>    vector[N] half_pred;</span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) W_tilde[n, n] <span class="sc">+</span><span class="er">=</span> <span class="dv">1</span>;</span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>    half_pred <span class="ot">=</span> W_tilde <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mdivide_left</span>(W_tilde, mu));</span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>    return <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log_determinant</span>(<span class="fu">crossprod</span>(W_tilde) <span class="sc">*</span> inv_sigma2) <span class="sc">-</span></span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a>           <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">dot_self</span>(half_pred) <span class="sc">*</span> inv_sigma2;</span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a>  }</span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a>}</span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a>data {</span>
<span id="cb16-27"><a href="#cb16-27" tabindex="-1"></a>  int<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">1</span><span class="sc">&gt;</span> N;  <span class="sc">/</span><span class="er">/</span> total number of observations</span>
<span id="cb16-28"><a href="#cb16-28" tabindex="-1"></a>  vector[N] Y;  <span class="sc">/</span><span class="er">/</span> response variable</span>
<span id="cb16-29"><a href="#cb16-29" tabindex="-1"></a>  int<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">0</span><span class="sc">&gt;</span> Nmi;  <span class="sc">/</span><span class="er">/</span> number of missings</span>
<span id="cb16-30"><a href="#cb16-30" tabindex="-1"></a>  int<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">1</span><span class="sc">&gt;</span> Jmi[Nmi];  <span class="sc">/</span><span class="er">/</span> positions of missings</span>
<span id="cb16-31"><a href="#cb16-31" tabindex="-1"></a>  int<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">1</span><span class="sc">&gt;</span> K;  <span class="sc">/</span><span class="er">/</span> number of population<span class="sc">-</span>level effects</span>
<span id="cb16-32"><a href="#cb16-32" tabindex="-1"></a>  matrix[N, K] X;  <span class="sc">/</span><span class="er">/</span> population<span class="sc">-</span>level design matrix</span>
<span id="cb16-33"><a href="#cb16-33" tabindex="-1"></a>  matrix[N, N] W;  <span class="sc">/</span><span class="er">/</span> spatial weight matrix</span>
<span id="cb16-34"><a href="#cb16-34" tabindex="-1"></a>  int prior_only;  <span class="sc">/</span><span class="er">/</span> should the likelihood be ignored?</span>
<span id="cb16-35"><a href="#cb16-35" tabindex="-1"></a>}</span>
<span id="cb16-36"><a href="#cb16-36" tabindex="-1"></a>transformed data {</span>
<span id="cb16-37"><a href="#cb16-37" tabindex="-1"></a>  int Kc <span class="ot">=</span> K <span class="sc">-</span> <span class="dv">1</span>;</span>
<span id="cb16-38"><a href="#cb16-38" tabindex="-1"></a>  matrix[N, K <span class="sc">-</span> <span class="dv">1</span>] Xc;  <span class="sc">/</span><span class="er">/</span> centered version of X</span>
<span id="cb16-39"><a href="#cb16-39" tabindex="-1"></a>  vector[K <span class="sc">-</span> <span class="dv">1</span>] means_X;  <span class="sc">/</span><span class="er">/</span> column means of X before centering</span>
<span id="cb16-40"><a href="#cb16-40" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>K) {</span>
<span id="cb16-41"><a href="#cb16-41" tabindex="-1"></a>    means_X[i <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">=</span> <span class="fu">mean</span>(X[, i]);</span>
<span id="cb16-42"><a href="#cb16-42" tabindex="-1"></a>    Xc[, i <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">=</span> X[, i] <span class="sc">-</span> means_X[i <span class="sc">-</span> <span class="dv">1</span>];</span>
<span id="cb16-43"><a href="#cb16-43" tabindex="-1"></a>  }</span>
<span id="cb16-44"><a href="#cb16-44" tabindex="-1"></a>}</span>
<span id="cb16-45"><a href="#cb16-45" tabindex="-1"></a>parameters {</span>
<span id="cb16-46"><a href="#cb16-46" tabindex="-1"></a>  vector[Nmi] Ymi;  <span class="sc">/</span><span class="er">/</span> estimated missings</span>
<span id="cb16-47"><a href="#cb16-47" tabindex="-1"></a>  vector[Kc] b;  <span class="sc">/</span><span class="er">/</span> population<span class="sc">-</span>level effects</span>
<span id="cb16-48"><a href="#cb16-48" tabindex="-1"></a>  real temp_Intercept;  <span class="sc">/</span><span class="er">/</span> temporary intercept</span>
<span id="cb16-49"><a href="#cb16-49" tabindex="-1"></a>  real<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">0</span><span class="sc">&gt;</span> sigma;  <span class="sc">/</span><span class="er">/</span> residual SD</span>
<span id="cb16-50"><a href="#cb16-50" tabindex="-1"></a>  real<span class="sc">&lt;</span>lower<span class="ot">=</span><span class="dv">0</span>,upper<span class="ot">=</span><span class="dv">1</span><span class="sc">&gt;</span> lagsar;  <span class="sc">/</span><span class="er">/</span> SAR parameter</span>
<span id="cb16-51"><a href="#cb16-51" tabindex="-1"></a>}</span>
<span id="cb16-52"><a href="#cb16-52" tabindex="-1"></a>transformed parameters {</span>
<span id="cb16-53"><a href="#cb16-53" tabindex="-1"></a>}</span>
<span id="cb16-54"><a href="#cb16-54" tabindex="-1"></a>model {</span>
<span id="cb16-55"><a href="#cb16-55" tabindex="-1"></a>  vector[N] Yl <span class="ot">=</span> Y;</span>
<span id="cb16-56"><a href="#cb16-56" tabindex="-1"></a>  vector[N] mu <span class="ot">=</span> Xc <span class="sc">*</span> b <span class="sc">+</span> temp_Intercept;</span>
<span id="cb16-57"><a href="#cb16-57" tabindex="-1"></a>  Yl[Jmi] <span class="ot">=</span> Ymi;</span>
<span id="cb16-58"><a href="#cb16-58" tabindex="-1"></a>  <span class="sc">/</span><span class="er">/</span> priors including all constants</span>
<span id="cb16-59"><a href="#cb16-59" tabindex="-1"></a>  target <span class="sc">+</span><span class="er">=</span> <span class="fu">student_t_lpdf</span>(temp_Intercept <span class="sc">|</span> <span class="dv">3</span>, <span class="dv">34</span>, <span class="dv">17</span>);</span>
<span id="cb16-60"><a href="#cb16-60" tabindex="-1"></a>  target <span class="sc">+</span><span class="er">=</span> <span class="fu">student_t_lpdf</span>(sigma <span class="sc">|</span> <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">17</span>)</span>
<span id="cb16-61"><a href="#cb16-61" tabindex="-1"></a>    <span class="sc">-</span> <span class="dv">1</span> <span class="sc">*</span> <span class="fu">student_t_lccdf</span>(<span class="dv">0</span> <span class="sc">|</span> <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">17</span>);</span>
<span id="cb16-62"><a href="#cb16-62" tabindex="-1"></a>  <span class="sc">/</span><span class="er">/</span> likelihood including all constants</span>
<span id="cb16-63"><a href="#cb16-63" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span>prior_only) {</span>
<span id="cb16-64"><a href="#cb16-64" tabindex="-1"></a>    target <span class="sc">+</span><span class="er">=</span> <span class="fu">normal_lagsar_lpdf</span>(Yl <span class="sc">|</span> mu, sigma, lagsar, W);</span>
<span id="cb16-65"><a href="#cb16-65" tabindex="-1"></a>  }</span>
<span id="cb16-66"><a href="#cb16-66" tabindex="-1"></a>}</span>
<span id="cb16-67"><a href="#cb16-67" tabindex="-1"></a>generated quantities {</span>
<span id="cb16-68"><a href="#cb16-68" tabindex="-1"></a>  <span class="sc">/</span><span class="er">/</span> actual population<span class="sc">-</span>level intercept</span>
<span id="cb16-69"><a href="#cb16-69" tabindex="-1"></a>  real b_Intercept <span class="ot">=</span> temp_Intercept <span class="sc">-</span> <span class="fu">dot_product</span>(means_X, b);</span>
<span id="cb16-70"><a href="#cb16-70" tabindex="-1"></a>}</span></code></pre></div>
<p>Here we want to focus on two aspects of the Stan code. First, because
there is no built-in function in Stan that calculates the log-likelihood
for the lag-SAR model, we define a new <code>normal_lagsar_lpdf</code>
function in the <code>functions</code> block of the Stan program. This
is the same function we showed earlier in the vignette and it can be
used to compute the log-likelihood in an efficient and numerically
stable way. The <code>_lpdf</code> suffix used in the function name
informs Stan that this is a log probability density function.</p>
<p>Second, this Stan program nicely illustrates how to set up missing
value imputation. Instead of just computing the log-likelihood for the
observed responses <code>Y</code>, we define a new variable
<code>Yl</code> which is equal to <code>Y</code> if the reponse is
observed and equal to <code>Ymi</code> if the response is missing. The
latter is in turn defined as a parameter and thus estimated along with
all other paramters of the model. More details about missing value
imputation in Stan can be found in the <em>Missing Data &amp; Partially
Known Parameters</em> section of the <a href="https://mc-stan.org/users/documentation/index.html" class="external-link">Stan
manual</a>.</p>
<p>The Stan code extracted from brms is not only helpful when learning
Stan, but can also drastically speed up the specification of models that
are not support by brms. If brms can fit a model similar but not
identical to the desired model, we can let brms generate the Stan
program for the similar model and then mold it into the program that
implements the model we actually want to fit. Rather than calling
<code><a href="https://paulbuerkner.com/brms/reference/stancode.html" class="external-link">stancode()</a></code>, which requires an existing fitted model object,
we recommend using <code><a href="https://paulbuerkner.com/brms/reference/stancode.html" class="external-link">make_stancode()</a></code> and specifying the
<code>save_model</code> argument to write the Stan program to a file.
The corresponding data can be prepared with <code><a href="https://paulbuerkner.com/brms/reference/standata.html" class="external-link">make_standata()</a></code>
and then manually amended if needed. Once the code and data have been
edited, they can be passed to RStan’s <code><a href="https://mc-stan.org/rstan/reference/stan.html" class="external-link">stan()</a></code> function via
the <code>file</code> and <code>data</code> arguments.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>In summary, we have shown how to set up and validate approximate and
exact LOO-CV for non-factorized multivariate normal models using Stan
with the <strong>brms</strong> and <strong>loo</strong> packages.
Although we focused on the particular example of a spatial SAR model,
the presented recipe applies more generally to models that can be
expressed in terms of a multivariate normal likelihood.</p>
<p><br></p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Anselin L. (1988). <em>Spatial econometrics: methods and models</em>.
Dordrecht: Kluwer Academic.</p>
<p>Bürkner P. C., Gabry J., &amp; Vehtari A. (2020). Efficient
leave-one-out cross-validation for Bayesian non-factorized normal and
Student-t models. <em>Computational Statistics</em>,
:10.1007/s00180-020-01045-4. <a href="https://arxiv.org/abs/1810.10559" class="external-link">ArXiv preprint</a>.</p>
<p>Sundararajan S. &amp; Keerthi S. S. (2001). Predictive approaches for
choosing hyperparameters in Gaussian processes. <em>Neural
Computation</em>, 13(5), 1103–1118.</p>
<p>Vehtari A., Mononen T., Tolvanen V., Sivula T., &amp; Winther O.
(2016). Bayesian leave-one-out cross-validation approximations for
Gaussian latent variable models. <em>Journal of Machine Learning
Research</em>, 17(103), 1–38. <a href="https://jmlr.org/papers/v17/14-540.html" class="external-link">Online</a>.</p>
<p>Vehtari A., Gelman A., &amp; Gabry J. (2017). Practical Bayesian
model evaluation using leave-one-out cross-validation and WAIC.
<em>Statistics and Computing</em>, 27(5), 1413–1432.
:10.1007/s11222-016-9696-4. <a href="https://link.springer.com/article/10.1007/s11222-016-9696-4" class="external-link">Online</a>.
<a href="https://arxiv.org/abs/1507.04544" class="external-link">arXiv preprint
arXiv:1507.04544</a>.</p>
<p>Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2024).
Pareto smoothed importance sampling. <em>Journal of Machine Learning
Research</em>, 25(72):1-58. <a href="https://jmlr.org/papers/v25/19-556.html" class="external-link">PDF</a></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Aki Vehtari, Jonah Gabry, Måns Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, Andrew Gelman.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

      </footer>
</div>






  </body>
</html>
